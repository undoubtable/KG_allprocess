"""
Step8 â€” ä½¿ç”¨ LLM ä»çŸ¥è¯†å›¾è°± + åŸå§‹å¥å­ç”Ÿæˆæ³•å¾‹å•é€‰é¢˜ï¼ˆMCQï¼‰

æ”¹åŠ¨è¦ç‚¹ï¼š
1. æ–°å¢è¯»å– Step2 å¥å­åˆ—è¡¨çš„ TSVï¼ˆåŒ…å« sentence_id å’ŒåŸå§‹ textï¼‰
2. åœ¨æ„é€  facts æ—¶ï¼ŒæŠŠ â€œKG è¾¹ + å¯¹åº”åŸå¥â€ åˆåœ¨ä¸€èµ·å–‚ç»™ LLM
3. prompt æ˜ç¡®è¦æ±‚ï¼šé¢˜ç›®å¿…é¡»èƒ½ä»ã€äº‹å® + åŸå¥ã€‘ä¸­æ¨å¯¼
"""

import csv
import os
import json
import time
from typing import List, Dict

from openai import OpenAI

# ========== è·¯å¾„é…ç½® ==========

# Step4 è¾“å‡ºï¼šKG èŠ‚ç‚¹ã€è¾¹
# NODES_TSV = r"D:\Desktop\KG_allprocess\KG_files\Output_files\Step4_output\ç¬¬ä¸€è®²_KG_nodes.tsv"
# EDGES_TSV = r"D:\Desktop\KG_allprocess\KG_files\Output_files\Step4_output\ç¬¬ä¸€è®²_KG_edges.tsv"

NODES_TSV = r"D:\Desktop\KG_allprocess\KG_files\Output_files\Step12_output\ç¬¬ä¸€è®²_KG_nodes_updated.tsv"
EDGES_TSV = r"D:\Desktop\KG_allprocess\KG_files\Output_files\Step12_output\ç¬¬ä¸€è®²_KG_edges_updated.tsv"

# âœ… æ–°å¢ï¼šStep2 è¾“å‡ºçš„å¥å­åˆ—è¡¨ï¼ˆéœ€è¦åŒ…å« sentence_id / page_no / textï¼‰
SENT_TSV = r"D:\Desktop\KG_allprocess\KG_files\Output_files\Step2_output\ç¬¬ä¸€è®²_å¥å­åˆ—è¡¨.tsv"

# ä½¿ç”¨ä½ ä¹‹å‰å†™å¥½çš„å•é€‰é¢˜ system prompt æ–‡ä»¶
PROMPT_PATH = r"D:\Desktop\KG_allprocess\KG_code\prompt.txt"

# è¾“å‡ºï¼šå•é€‰é¢˜ TSV
# OUTPUT_Q_TSV = r"D:\Desktop\KG_allprocess\KG_files\Output_files\Step8_output\ç¬¬ä¸€è®²_MCQ.tsv"

OUTPUT_Q_TSV = r"D:\Desktop\KG_allprocess\KG_files\Output_files\Step8_output\ç¬¬ä¸€è®²_MCQ_updated.tsv"


# ========== LLM é…ç½® ==========

# âœ… å»ºè®®ä½¿ç”¨ç¯å¢ƒå˜é‡å­˜ keyï¼Œé¿å…æ˜æ–‡å†™æ­»åœ¨ä»£ç é‡Œ
#   ä½ å¯ä»¥ï¼š
#   - åœ¨ç³»ç»Ÿç¯å¢ƒå˜é‡é‡Œè®¾ç½®ï¼šGITEE_AI_API_KEY=ä½ çš„key
#   - æˆ–è€…ç›´æ¥æŠŠ os.getenv(...) æ›¿æ¢æˆ "ä½ çš„çœŸå® key"ï¼Œä¾‹å¦‚ï¼š
#       api_key="DUxxxxxxxxxxxxxxxxxxxx"
import yaml

with open("config.yaml", "r", encoding="utf-8") as f:
    config = yaml.safe_load(f)

# ========== LLM é…ç½® ==========

client = OpenAI(
    base_url="https://ai.gitee.com/v1",
    api_key = config["api_key"],
    default_headers={"X-Failover-Enabled": "true"},
)

# æ¨¡å‹åç§°ï¼šæŒ‰ä½ åœ¨ gitee ä¸ŠçœŸå®å¯ç”¨çš„æ¨¡å‹åç§°å¡«å†™
MODEL_NAME = "DeepSeek-R1"  # TODO: å¦‚æœ‰éœ€è¦å¯ä»¥ä¿®æ”¹æˆå…¶ä»–æ¨¡å‹


# ========== ç”Ÿæˆç­–ç•¥å‚æ•° ==========

MAX_QUESTIONS = 50          # æœ€å¤šç”Ÿæˆå¤šå°‘é¢˜
EDGES_PER_CHUNK = 5         # æ¯æ¬¡ç»™ LLM çš„ fact æ¡æ•°
QUESTIONS_PER_CHUNK = 3     # æ¯ä¸ª chunk æœŸæœ›ç”Ÿæˆå‡ é“é¢˜ï¼ˆä¸Šé™å€¼ï¼‰


# ========== å·¥å…·å‡½æ•° ==========

def load_prompt_text(path: str) -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read()


SYSTEM_PROMPT = load_prompt_text(PROMPT_PATH)


def load_nodes(path: str) -> Dict[str, Dict]:
    """è¯»å–èŠ‚ç‚¹ TSVï¼šnode_id -> row"""
    nodes: Dict[str, Dict] = {}
    with open(path, encoding="utf-8") as f:
        reader = csv.DictReader(f, delimiter="\t")
        for r in reader:
            nodes[r["node_id"]] = r
    return nodes


def load_edges(path: str) -> List[Dict]:
    """è¯»å–è¾¹ TSVï¼šè¿”å›åˆ—è¡¨"""
    edges: List[Dict] = []
    with open(path, encoding="utf-8") as f:
        reader = csv.DictReader(f, delimiter="\t")
        for r in reader:
            edges.append(r)
    return edges


def load_sentences(path: str) -> Dict[str, Dict]:
    """
    è¯»å– Step2 çš„å¥å­ TSVï¼š
    å‡è®¾åˆ—é¡ºåºä¸ºï¼šsentence_id | page_no | text
    ï¼ˆå’Œä½  Step3 çš„è¯»å–æ–¹å¼ä¿æŒä¸€è‡´ï¼Œä¸ä¾èµ–åˆ—åï¼‰
    """
    sentences: Dict[str, Dict] = {}
    if not os.path.exists(path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°å¥å­ TSV æ–‡ä»¶ï¼š{path}")

    with open(path, "r", encoding="utf-8") as f:
        header = f.readline()  # è·³è¿‡è¡¨å¤´
        for line in f:
            line = line.rstrip("\n")
            if not line:
                continue
            parts = line.split("\t")
            if len(parts) < 3:
                continue
            sentence_id = parts[0]
            page_no = parts[1]
            text = parts[2]
            sentences[sentence_id] = {
                "sentence_id": sentence_id,
                "page_no": page_no,
                "text": text,
            }
    return sentences


def build_fact_strings(
    nodes: Dict[str, Dict],
    edges: List[Dict],
    sentences: Dict[str, Dict],
) -> List[str]:
    """
    æŠŠ KG çš„è¾¹ + å¯¹åº”åŸå¥ä¸€èµ·å˜æˆâ€œå¯è¯»äº‹å®å­—ç¬¦ä¸²â€ï¼Œå–‚ç»™ LLMã€‚

    æ ¼å¼ç¤ºä¾‹ï¼š
      äº‹å®ï¼šå…¨å›½äººå¤§å¸¸å§”ä¼š --related_to--> åˆ‘æ³•ä¿®æ­£æ¡ˆ(å…«)
      æ¥æºåŸå¥ï¼šå…¨å›½äººå¤§å¸¸å§”ä¼šé€šè¿‡äº†ã€Šåˆ‘æ³•ä¿®æ­£æ¡ˆ(å…«)ã€‹ã€‚

    å¦‚æœæ‰¾ä¸åˆ° sentence_id å¯¹åº”çš„åŸå¥ï¼Œåˆ™åªç»™äº‹å®è¡Œã€‚
    """
    facts: List[str] = []
    for e in edges:
        src_name = nodes.get(e["src_id"], {}).get("name", e["src_id"])
        dst_name = nodes.get(e["dst_id"], {}).get("name", e["dst_id"])
        rel = e.get("relation_type", "related_to")

        sent_id = e.get("sentence_id", "")
        sent_text = sentences.get(sent_id, {}).get("text", "").strip()

        if sent_text:
            fact = (
                f"äº‹å®ï¼š{src_name} --{rel}--> {dst_name}\n"
                f"æ¥æºåŸå¥ï¼š{sent_text}"
            )
        else:
            fact = f"äº‹å®ï¼š{src_name} --{rel}--> {dst_name}"

        facts.append(fact)
    return facts


def chunk_list(lst: List[str], size: int) -> List[List[str]]:
    """
    æŠŠåˆ—è¡¨æŒ‰å›ºå®šå¤§å°åˆ‡åˆ†æˆå¤šä¸ªå°å—ã€‚
    è¿”å› List[List[str]]ã€‚
    """
    chunks: List[List[str]] = []
    for i in range(0, len(lst), size):
        chunks.append(lst[i:i + size])
    return chunks


# ========== LLM è°ƒç”¨ï¼šç”Ÿæˆ MCQ ==========

def call_llm_for_mcq(fact_chunk: List[str], n_questions: int) -> List[Dict]:
    """
    è°ƒç”¨ LLMï¼šåŸºäºä¸€ä¸ª fact_chunk ç”Ÿæˆè‹¥å¹²é“å•é€‰é¢˜ã€‚

    è¿”å›æ ¼å¼ï¼š
    [
      {
        "question": "...",
        "options": ["A. ...", "B. ...", "C. ...", "D. ..."],
        "answer": "B"
      },
      ...
    ]
    """
    if not fact_chunk or n_questions <= 0:
        return []

    # åŠ ç¼–å·ï¼Œæ–¹ä¾¿æ¨¡å‹é˜…è¯»
    facts_text = "\n".join(
        f"{idx + 1}. {f}" for idx, f in enumerate(fact_chunk)
    )

    user_prompt = f"""
ä¸‹é¢æ˜¯è‹¥å¹²æ¡æ¥è‡ªæ³•å¾‹çŸ¥è¯†å›¾è°±çš„â€œäº‹å®åŠå…¶æ¥æºåŸå¥â€ï¼š

{facts_text}

è¯´æ˜ï¼š
- æ¯æ¡è®°å½•é€šå¸¸åŒ…å«ä¸¤è¡Œï¼š
  - â€œäº‹å®ï¼š...â€ è¡Œæè¿°çŸ¥è¯†å›¾è°±ä¸­çš„å®ä½“å…³ç³»
  - â€œæ¥æºåŸå¥ï¼š...â€ è¡Œç»™å‡ºè¯¥äº‹å®åœ¨åŸå§‹ææ–™ä¸­çš„å®Œæ•´å¥å­ï¼ˆè‹¥æœ‰ï¼‰

è¯·ä½ ã€ä»…æ ¹æ®ä¸Šè¿°äº‹å®åŠåŸå¥ã€‘ç”Ÿæˆ {n_questions} é“ä¸­æ–‡æ³•å¾‹å•é€‰é¢˜ï¼ˆMultiple Choice Questionsï¼ŒMCQï¼‰ï¼Œå¹¶æ»¡è¶³ä»¥ä¸‹è¦æ±‚ï¼š

1. æ¯é“é¢˜å¿…é¡»åŒ…å«å­—æ®µï¼š
   - "question"ï¼šé¢˜å¹²ï¼ˆç”¨ä¸­æ–‡è¡¨è¿°ï¼Œå¯é€‚å½“æ”¹å†™åŸå¥ï¼Œä½†ä¸èƒ½è„±ç¦»åŸæ„ï¼‰
   - "options"ï¼šåŒ…å«å››ä¸ªå…ƒç´ çš„æ•°ç»„ ["A. ...", "B. ...", "C. ...", "D. ..."]
   - "answer"ï¼šæ­£ç¡®é€‰é¡¹çš„é€‰é¡¹å­—æ¯ï¼ˆåªèƒ½æ˜¯ "A"ã€"B"ã€"C" æˆ– "D"ï¼‰
2. é¢˜ç›®å†…å®¹å’Œé€‰é¡¹å¿…é¡»èƒ½å¤Ÿä»ã€äº‹å® + åŸå¥ã€‘ä¸­æ¨å¯¼å‡ºæ¥ï¼Œä¸å…è®¸å¼•å…¥ææ–™ä¸­æ²¡æœ‰çš„ä¿¡æ¯ã€‚
3. å››ä¸ªé€‰é¡¹éƒ½è¦åˆç†ã€æœ‰ä¸€å®šè¿·æƒ‘æ€§ï¼Œä½†ä¸èƒ½æ˜æ˜¾é”™è¯¯æˆ–ä¸åŸå¥çŸ›ç›¾ã€‚
4. æ¯é“é¢˜åªèƒ½æœ‰ä¸€ä¸ªå”¯ä¸€æ­£ç¡®ç­”æ¡ˆï¼Œä¸è¦å‡ºç°å¤šé€‰æˆ–æ¨¡ç³Šä¸æ¸…çš„æƒ…å†µã€‚
5. ä¸è¦è¾“å‡ºé¢˜ç›®è§£ææˆ–ä»»ä½•è§£é‡Šã€‚
6. æœ€ç»ˆè¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ª JSON æ•°ç»„ï¼Œä¾‹å¦‚ï¼š
   [
     {{"question": "...", "options": ["A. ...", "B. ...", "C. ...", "D. ..."], "answer": "B"}},
     ...
   ]
7. ä¸è¦åœ¨ JSON å¤–è¾“å‡ºä»»ä½•é¢å¤–æ–‡å­—ï¼ˆä¸è¦ markdownã€ä¸è¦è¯´æ˜ï¼Œåªè¦ JSONï¼‰ã€‚
"""

    response = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0.7,
    )

    content = response.choices[0].message.content.strip()

    # è§£æ JSONï¼ˆå…¼å®¹æ¨¡å‹å¯èƒ½åŠ çš„ ```json ... ``` åŒ…è£¹ï¼‰
    try:
        if content.startswith("```"):
            # å»æ‰å¯èƒ½çš„ ```json / ``` åŒ…è£¹
            content = content.strip("`")
            idx = content.find("[")
            if idx != -1:
                content = content[idx:]

        start = content.find("[")
        end = content.rfind("]")
        if start != -1 and end != -1:
            content = content[start: end + 1]

        data = json.loads(content)
        mcqs: List[Dict] = []

        if isinstance(data, list):
            for item in data:
                if not isinstance(item, dict):
                    continue
                q = str(item.get("question", "")).strip()
                options = item.get("options", [])
                answer = str(item.get("answer", "")).strip()

                if not q or not isinstance(options, list) or len(options) != 4:
                    continue

                options = [str(opt).strip() for opt in options]
                if answer not in ("A", "B", "C", "D"):
                    continue

                mcqs.append(
                    {
                        "question": q,
                        "options": options,
                        "answer": answer,
                    }
                )

        return mcqs[:n_questions]

    except Exception as e:
        print("âš  è§£æ LLM è¾“å‡º JSON å¤±è´¥ï¼š", e)
        print("åŸå§‹å†…å®¹ç‰‡æ®µï¼š", content[:300], "...")
        return []


# ========== ä¸»é€»è¾‘ï¼šåˆ†å—ç”Ÿæˆ MCQ + è¿›åº¦æ˜¾ç¤º ==========

def generate_mcq_with_llm(
    nodes: Dict[str, Dict],
    edges: List[Dict],
    sentences: Dict[str, Dict],
) -> List[Dict]:
    """
    åˆ†å—å–‚ factsï¼ˆKG è¾¹ + åŸå¥ï¼‰ï¼Œè°ƒç”¨ LLM ç”Ÿæˆå¤šé“å•é€‰é¢˜ã€‚

    è¿”å›æ ¼å¼ï¼š
    [
      {
        "qid": "q0001",
        "question": "...",
        "option_a": "...",
        "option_b": "...",
        "option_c": "...",
        "option_d": "...",
        "answer": "B"
      },
      ...
    ]
    """
    facts = build_fact_strings(nodes, edges, sentences)
    chunks = chunk_list(facts, EDGES_PER_CHUNK)

    all_mcq_rows: List[Dict] = []
    q_counter = 1
    total_chunks = len(chunks)
    avg_call_time: List[float] = []

    print(f"\nğŸ”„ å…± {total_chunks} ä¸ª fact chunkï¼Œå°†ç”Ÿæˆæœ€å¤š {MAX_QUESTIONS} é“é¢˜\n")

    for idx, fact_chunk in enumerate(chunks, start=1):
        if len(all_mcq_rows) >= MAX_QUESTIONS:
            break

        remain = MAX_QUESTIONS - len(all_mcq_rows)
        n_q = min(QUESTIONS_PER_CHUNK, remain)

        print(f"\nğŸ“Œ Chunk {idx}/{total_chunks}: å°è¯•ç”Ÿæˆ {n_q} é“é¢˜")
        print("   ğŸ¤– è°ƒç”¨ LLM ä¸­â€¦â€¦")

        start_time = time.time()
        mcqs = call_llm_for_mcq(fact_chunk, n_q)
        cost = time.time() - start_time
        avg_call_time.append(cost)

        print(f"   âœ… LLM è¿”å›ï¼ˆè€—æ—¶ {cost:.2f} ç§’ï¼‰")

        for item in mcqs:
            qid = f"q{q_counter:04d}"
            options = item["options"]
            row = {
                "qid": qid,
                "question": item["question"],
                "option_a": options[0],
                "option_b": options[1],
                "option_c": options[2],
                "option_d": options[3],
                "answer": item["answer"],
            }
            all_mcq_rows.append(row)
            q_counter += 1

        # ç®€å• ETA ä¼°è®¡
        progress = len(all_mcq_rows)
        if avg_call_time and progress > 0:
            avg_t = sum(avg_call_time) / len(avg_call_time)
            # å‰©ä½™è¦è°ƒç”¨å‡ æ¬¡ LLM â‰ˆ å‰©ä½™é¢˜æ•° / æ¯æ¬¡ç”Ÿæˆé¢˜æ•°
            remain_calls = max(
                (MAX_QUESTIONS - progress) / max(QUESTIONS_PER_CHUNK, 1),
                0,
            )
            eta = remain_calls * avg_t
            print(
                f"   ğŸ“Š è¿›åº¦ï¼šå·²ç”Ÿæˆ {progress}/{MAX_QUESTIONS} é“é¢˜ | "
                f"é¢„ä¼°å‰©ä½™æ—¶é—´çº¦ï¼š{eta:.1f} ç§’"
            )

    print("\nğŸ‰ é¢˜ç›®ç”Ÿæˆå®Œæˆï¼")
    return all_mcq_rows


# ========== ä¿å­˜ TSV ==========

def save_mcq(rows: List[Dict], path: str):
    """ä¿å­˜ TSVï¼šqid, question, option_a, option_b, option_c, option_d, answer"""
    if not rows:
        print("âš  æ²¡æœ‰ç”Ÿæˆä»»ä½•é¢˜ç›®ï¼Œæ–‡ä»¶ä¸ä¼šå†™å‡ºã€‚")
        return

    os.makedirs(os.path.dirname(path), exist_ok=True)
    fieldnames = [
        "qid",
        "question",
        "option_a",
        "option_b",
        "option_c",
        "option_d",
        "answer",
    ]

    with open(path, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter="\t")
        writer.writeheader()
        for r in rows:
            writer.writerow(r)

    print(f"\nâœ… å·²ä¿å­˜å•é€‰é¢˜ï¼š{path}ï¼ˆå…± {len(rows)} é¢˜ï¼‰")


# ========== main ==========

def main():
    if not os.path.exists(NODES_TSV) or not os.path.exists(EDGES_TSV):
        raise FileNotFoundError("è¯·æ£€æŸ¥èŠ‚ç‚¹/è¾¹ TSV è·¯å¾„æ˜¯å¦æ­£ç¡®")
    if not os.path.exists(SENT_TSV):
        raise FileNotFoundError("è¯·æ£€æŸ¥å¥å­ TSV è·¯å¾„æ˜¯å¦æ­£ç¡®")

    nodes = load_nodes(NODES_TSV)
    edges = load_edges(EDGES_TSV)
    sentences = load_sentences(SENT_TSV)

    print(f"ğŸ“„ å·²åŠ è½½èŠ‚ç‚¹æ•°ï¼š{len(nodes)}ï¼Œè¾¹æ•°ï¼š{len(edges)}ï¼Œå¥å­æ•°ï¼š{len(sentences)}")

    mcq_rows = generate_mcq_with_llm(nodes, edges, sentences)
    save_mcq(mcq_rows, OUTPUT_Q_TSV)


if __name__ == "__main__":
    main()
