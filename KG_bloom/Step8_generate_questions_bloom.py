"""
Step8 â€” ä½¿ç”¨ LLM ä»çŸ¥è¯†å›¾è°± + åŸå§‹å¥å­ç”Ÿæˆæ³•å¾‹å•é€‰é¢˜ï¼ˆMCQï¼‰
æ–°å¢ï¼šåŸºäºå¸ƒé²å§†åˆ†ç±»ï¼ˆBloom's Taxonomyï¼‰ç”Ÿæˆé¢˜ç›®ï¼Œå¹¶åœ¨è¾“å‡ºä¸­æ ‡æ³¨ bloom_level / bloom_label

è¦ç‚¹ï¼š
1) facts = KG è¾¹ + å¯¹åº”åŸå¥
2) LLM è¾“å‡ºæ–°å¢ bloom_level/bloom_label
3) æ¯ä¸ª chunk 3 é¢˜ï¼šå¼ºåˆ¶è¦†ç›–ä¸åŒ Bloomï¼ˆé»˜è®¤ B1/B3/B4 è½®æ¢ï¼‰
4) è¾“å‡º TSV å¢åŠ  bloom_levelã€bloom_label ä¸¤åˆ—
"""

import csv
import os
import json
import time
from typing import List, Dict, Optional

from openai import OpenAI
from pipeline_config import STEP4_NODES_TSV, STEP4_EDGES_TSV, STEP2_SENT_TSV
from pipeline_config import STEP8_Q_TSV, PROMPT_PATH_BLOOM

NODES_TSV = str(STEP4_NODES_TSV)
EDGES_TSV = str(STEP4_EDGES_TSV)
SENT_TSV = str(STEP2_SENT_TSV)
OUTPUT_Q_TSV = str(STEP8_Q_TSV)

# ========== è·¯å¾„é…ç½® ==========

# NODES_TSV = r"D:\Desktop\KG_allprocess\KG_files\Output_files\Step12_output\ç¬¬ä¸€è®²_KG_nodes_updated.tsv"
# EDGES_TSV = r"D:\Desktop\KG_allprocess\KG_files\Output_files\Step12_output\ç¬¬ä¸€è®²_KG_edges_updated.tsv"

# SENT_TSV = r"D:\Desktop\KG_allprocess\KG_files\Output_files\Step2_output\ç¬¬ä¸€è®²_å¥å­åˆ—è¡¨.tsv"

# âœ… æ”¹ï¼šä½¿ç”¨æ–°çš„ system promptï¼ˆè§ä¸‹æ–¹æç¤ºè¯å†…å®¹ï¼‰
# PROMPT_PATH = r"D:\Desktop\KG_allprocess\KG_code\prompt_bloom_same_knowledge.txt"
PROMPT_PATH = str(PROMPT_PATH_BLOOM)
# OUTPUT_Q_TSV = r"D:\Desktop\KG_allprocess\KG_files\Output_files\Step8_output\ç¬¬ä¸€è®²_MCQ_bloom.tsv"


# ========== LLM é…ç½® ==========

import yaml

with open("config.yaml", "r", encoding="utf-8") as f:
    config = yaml.safe_load(f)

# ========== LLM é…ç½® ==========

client = OpenAI(
    base_url="https://ai.gitee.com/v1",
    api_key = config["api_key"],
    default_headers={"X-Failover-Enabled": "true"},
)

MODEL_NAME = "DeepSeek-R1"


# ========== ç”Ÿæˆç­–ç•¥å‚æ•° ==========

MAX_QUESTIONS = 50          # æœ€å¤šç”Ÿæˆå¤šå°‘é¢˜
EDGES_PER_CHUNK = 5         # æ¯æ¬¡ç»™ LLM çš„ fact æ¡æ•°
QUESTIONS_PER_CHUNK = 6     # æ¯ä¸ª chunk æœŸæœ›ç”Ÿæˆå‡ é“é¢˜ï¼ˆä¸Šé™å€¼ï¼‰

# âœ… æ¯æ¬¡ chunk 3 é¢˜çš„ Bloom ç›®æ ‡åˆ†å¸ƒï¼ˆå¼ºçº¦æŸï¼‰
# ä½ ä¹Ÿå¯ä»¥æ”¹æˆ ["B1","B2","B3"] æˆ– ["B2","B4","B5"] ç­‰
BLOOM_PATTERN = ["B1", "B2", "B3", "B4", "B5", "B6"]

# âœ… Bloom æ˜ å°„
BLOOM_LEVELS = {
    "B1": "è®°å¿†",
    "B2": "ç†è§£",
    "B3": "åº”ç”¨",
    "B4": "åˆ†æ",
    "B5": "è¯„ä»·",
    "B6": "åˆ›é€ ",
}


# ========== å·¥å…·å‡½æ•° ==========

def load_prompt_text(path: str) -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read()


SYSTEM_PROMPT = load_prompt_text(PROMPT_PATH)


def load_nodes(path: str) -> Dict[str, Dict]:
    """è¯»å–èŠ‚ç‚¹ TSVï¼šnode_id -> row"""
    nodes: Dict[str, Dict] = {}
    with open(path, encoding="utf-8") as f:
        reader = csv.DictReader(f, delimiter="\t")
        for r in reader:
            nodes[r["node_id"]] = r
    return nodes


def load_edges(path: str) -> List[Dict]:
    """è¯»å–è¾¹ TSVï¼šè¿”å›åˆ—è¡¨"""
    edges: List[Dict] = []
    with open(path, encoding="utf-8") as f:
        reader = csv.DictReader(f, delimiter="\t")
        for r in reader:
            edges.append(r)
    return edges


def load_sentences(path: str) -> Dict[str, Dict]:
    """
    è¯»å– Step2 çš„å¥å­ TSVï¼š
    å‡è®¾åˆ—é¡ºåºä¸ºï¼šsentence_id | page_no | text
    ï¼ˆä¸ä¾èµ–åˆ—åï¼‰
    """
    sentences: Dict[str, Dict] = {}
    if not os.path.exists(path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°å¥å­ TSV æ–‡ä»¶ï¼š{path}")

    with open(path, "r", encoding="utf-8") as f:
        _ = f.readline()  # è·³è¿‡è¡¨å¤´
        for line in f:
            line = line.rstrip("\n")
            if not line:
                continue
            parts = line.split("\t")
            if len(parts) < 3:
                continue
            sentence_id = parts[0]
            page_no = parts[1]
            text = parts[2]
            sentences[sentence_id] = {
                "sentence_id": sentence_id,
                "page_no": page_no,
                "text": text,
            }
    return sentences


def build_fact_strings(
    nodes: Dict[str, Dict],
    edges: List[Dict],
    sentences: Dict[str, Dict],
) -> List[str]:
    """
    æŠŠ KG çš„è¾¹ + å¯¹åº”åŸå¥ä¸€èµ·å˜æˆâ€œå¯è¯»äº‹å®å­—ç¬¦ä¸²â€ï¼Œå–‚ç»™ LLMã€‚
    """
    facts: List[str] = []
    for e in edges:
        src_name = nodes.get(e["src_id"], {}).get("name", e["src_id"])
        dst_name = nodes.get(e["dst_id"], {}).get("name", e["dst_id"])
        rel = e.get("relation_type", "related_to")

        sent_id = e.get("sentence_id", "")
        sent_text = sentences.get(sent_id, {}).get("text", "").strip()

        if sent_text:
            fact = (
                f"äº‹å®ï¼š{src_name} --{rel}--> {dst_name}\n"
                f"æ¥æºåŸå¥ï¼š{sent_text}"
            )
        else:
            fact = f"äº‹å®ï¼š{src_name} --{rel}--> {dst_name}"

        facts.append(fact)
    return facts


def chunk_list(lst: List[str], size: int) -> List[List[str]]:
    """æŠŠåˆ—è¡¨æŒ‰å›ºå®šå¤§å°åˆ‡åˆ†æˆå¤šä¸ªå°å—ã€‚"""
    return [lst[i:i + size] for i in range(0, len(lst), size)]


def extract_json_array(text: str) -> Optional[str]:
    """
    ä»æ¨¡å‹è¾“å‡ºé‡Œå°½é‡æå– JSON æ•°ç»„éƒ¨åˆ†ï¼š[ ... ]
    å…¼å®¹ ```json ... ``` æˆ–å‰åæœ‰æ‚è®¯ã€‚
    """
    if not text:
        return None

    t = text.strip()

    # å»æ‰æœ€å¤–å±‚å¯èƒ½çš„ ``` åŒ…è£¹
    if t.startswith("```"):
        # å¯èƒ½æ˜¯ ```json\n...\n```
        t = t.strip("`").strip()
        # å¦‚æœè¿˜åŒ…å« 'json' æ ‡è®°ï¼Œå»æ‰å…¶å‰ç¼€è¡Œ
        if t.lower().startswith("json"):
            t = t[4:].strip()

    start = t.find("[")
    end = t.rfind("]")
    if start == -1 or end == -1 or end <= start:
        return None
    return t[start:end + 1]


def bloom_targets_for_chunk(chunk_index_1based: int, n_questions: int) -> List[str]:
    """
    ç»™ç¬¬ chunk_index ä¸ª chunk åˆ†é… bloom ç›®æ ‡ï¼ˆå¼ºçº¦æŸï¼‰ï¼ŒæŒ‰ BLOOM_PATTERN è½®æ¢ã€‚
    é»˜è®¤æ¯ chunk 3 é¢˜ï¼š
      chunk1: B1,B3,B4
      chunk2: B1,B3,B4
    ä½ ä¹Ÿå¯ä»¥æ‰©å±•æˆéš chunk å˜åŒ–çš„æ¨¡å¼ã€‚
    """
    # æœ€ç®€å•ï¼šå›ºå®šæ¨¡å¼æˆªå– n_questions
    pattern = BLOOM_PATTERN[:]
    if n_questions <= len(pattern):
        return pattern[:n_questions]

    # è‹¥ä½ è®¾äº† n_questions > pattern é•¿åº¦ï¼Œåˆ™å¾ªç¯è¡¥é½
    out = []
    while len(out) < n_questions:
        out.extend(pattern)
    return out[:n_questions]


# ========== LLM è°ƒç”¨ï¼šç”Ÿæˆ MCQï¼ˆå¸¦ Bloomï¼‰ ==========

def call_llm_for_mcq(fact_chunk: List[str], target_blooms: List[str]) -> List[Dict]:
    """
    è°ƒç”¨ LLMï¼šåŸºäºä¸€ä¸ª fact_chunk ç”Ÿæˆè‹¥å¹²é“å•é€‰é¢˜ï¼Œå¹¶å¼ºåˆ¶ Bloom å±‚çº§ã€‚
    è¿”å› item éœ€åŒ…å«ï¼š
      question/options/answer/bloom_level/bloom_label
    """
    n_questions = len(target_blooms)
    if not fact_chunk or n_questions <= 0:
        return []

    # ç»™ fact ç¼–å·
    facts_text = "\n".join(f"{idx + 1}. {f}" for idx, f in enumerate(fact_chunk))

    # ç»™ Bloom ç›®æ ‡ç¼–å·ï¼Œå¼ºåˆ¶æ¯é¢˜ä¸€ä¸ªå±‚çº§
    bloom_req_text = "\n".join(
        f"- ç¬¬{i+1}é¢˜ bloom_level å¿…é¡»æ˜¯ {b}ï¼ˆ{BLOOM_LEVELS[b]}ï¼‰"
        for i, b in enumerate(target_blooms)
        if b in BLOOM_LEVELS
    )

    user_prompt = f"""
ä¸‹é¢æ˜¯è‹¥å¹²æ¡æ¥è‡ªæ³•å¾‹çŸ¥è¯†å›¾è°±çš„â€œäº‹å®åŠå…¶æ¥æºåŸå¥â€ï¼š

{facts_text}

è¯´æ˜ï¼š
- æ¯æ¡è®°å½•é€šå¸¸åŒ…å«ä¸¤è¡Œï¼š
  - â€œäº‹å®ï¼š...â€ è¡Œæè¿°çŸ¥è¯†å›¾è°±ä¸­çš„å®ä½“å…³ç³»
  - â€œæ¥æºåŸå¥ï¼š...â€ è¡Œç»™å‡ºè¯¥äº‹å®åœ¨åŸå§‹ææ–™ä¸­çš„å®Œæ•´å¥å­ï¼ˆè‹¥æœ‰ï¼‰

è¯·ä½ ã€ä»…æ ¹æ®ä¸Šè¿°äº‹å®åŠåŸå¥ã€‘ç”Ÿæˆ {n_questions} é“ä¸­æ–‡æ³•å¾‹å•é€‰é¢˜ï¼ˆMCQï¼‰ï¼Œå¹¶æ»¡è¶³ä»¥ä¸‹è¦æ±‚ï¼š

1. æ¯é“é¢˜å¿…é¡»åŒ…å«å­—æ®µï¼š
   - "question"ï¼šé¢˜å¹²ï¼ˆä¸­æ–‡è¡¨è¿°ï¼Œå¯é€‚å½“æ”¹å†™åŸå¥ï¼Œä½†ä¸èƒ½è„±ç¦»åŸæ„ï¼‰
   - "options"ï¼šåŒ…å«å››ä¸ªå…ƒç´ çš„æ•°ç»„ ["A. ...", "B. ...", "C. ...", "D. ..."]
   - "answer"ï¼šæ­£ç¡®é€‰é¡¹çš„é€‰é¡¹å­—æ¯ï¼ˆåªèƒ½æ˜¯ "A"ã€"B"ã€"C" æˆ– "D"ï¼‰
   - "bloom_level"ï¼šåªèƒ½æ˜¯ ["B1","B2","B3","B4","B5","B6"] ä¹‹ä¸€
   - "bloom_label"ï¼šåªèƒ½æ˜¯ ["è®°å¿†","ç†è§£","åº”ç”¨","åˆ†æ","è¯„ä»·","åˆ›é€ "] ä¹‹ä¸€

2. é¢˜ç›®å†…å®¹å’Œé€‰é¡¹å¿…é¡»èƒ½å¤Ÿä»ã€äº‹å® + åŸå¥ã€‘ä¸­æ¨å¯¼å‡ºæ¥ï¼Œä¸å…è®¸å¼•å…¥ææ–™ä¸­æ²¡æœ‰çš„ä¿¡æ¯ï¼ˆåŒ…æ‹¬å¸¸è¯†æ€§æ³•å¾‹çŸ¥è¯†ã€èƒŒæ™¯æ³•æ¡ã€æ¨æµ‹æ€§ç»“è®ºï¼‰ã€‚
3. å››ä¸ªé€‰é¡¹éƒ½è¦åˆç†ã€æœ‰ä¸€å®šè¿·æƒ‘æ€§ï¼Œä½†ä¸èƒ½æ˜æ˜¾é”™è¯¯æˆ–ä¸åŸå¥çŸ›ç›¾ã€‚
4. æ¯é“é¢˜åªèƒ½æœ‰ä¸€ä¸ªå”¯ä¸€æ­£ç¡®ç­”æ¡ˆï¼Œä¸è¦å‡ºç°å¤šé€‰æˆ–æ¨¡ç³Šä¸æ¸…çš„æƒ…å†µã€‚
5. ä¸è¦è¾“å‡ºé¢˜ç›®è§£ææˆ–ä»»ä½•è§£é‡Šã€‚
6. æœ€ç»ˆè¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ª JSON æ•°ç»„ï¼Œä¾‹å¦‚ï¼š
   [
     {{"question":"...","options":["A. ...","B. ...","C. ...","D. ..."],"answer":"B","bloom_level":"B2","bloom_label":"ç†è§£"}},
     ...
   ]
7. ä¸è¦åœ¨ JSON å¤–è¾“å‡ºä»»ä½•é¢å¤–æ–‡å­—ï¼ˆä¸è¦ markdownã€ä¸è¦è¯´æ˜ï¼Œåªè¦ JSONï¼‰ã€‚

ã€å¸ƒé²å§†å±‚çº§çº¦æŸï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰ã€‘
- B1 è®°å¿†ï¼šè€ƒå¯Ÿå¯¹åŸå¥/äº‹å®çš„ç›´æ¥å›å¿†æˆ–è¯†åˆ«ã€‚
- B2 ç†è§£ï¼šè€ƒå¯Ÿé‡Šä¹‰ã€åŒä¹‰æ”¹å†™ã€æ¦‚æ‹¬å½’çº³ï¼ˆä»å®Œå…¨å¿ äºææ–™ï¼‰ã€‚
- B3 åº”ç”¨ï¼šå°†ææ–™ä¸­è¡¨è¾¾çš„å…³ç³»/è§„åˆ™ç”¨äºâ€œææ–™å†…éƒ¨ç­‰ä»·è¡¨è¿°â€çš„åˆ¤æ–­ï¼›ä¸å¾—å¼•å…¥æ–°ä¸»ä½“/æ–°æ¡ä»¶/æ–°ç»“è®ºã€‚
- B4 åˆ†æï¼šæ‹†åˆ†è¦ç´ ã€è¾¨æå…³ç³»ï¼ˆä¸»ä½“/å®¢ä½“/å…³ç³»ç±»å‹/æ¡ä»¶-ç»“è®ºç»“æ„ï¼‰ï¼Œä»ä¸å¾—å¼•å…¥ææ–™å¤–ä¿¡æ¯ã€‚
- B5 è¯„ä»·ï¼šä»…åœ¨ææ–™å†…éƒ¨åšä¸€è‡´æ€§/è´´åˆåº¦åˆ¤æ–­ï¼›ä¸å¾—ç”¨ææ–™å¤–æ ‡å‡†æˆ–ä»·å€¼åˆ¤æ–­ä¾æ®ã€‚
- B6 åˆ›é€ ï¼šåªèƒ½åšâ€œåŸºäºææ–™çš„é‡ç»„è¡¨è¾¾/è§„èŒƒåŒ–è¡¨è¿°/æŠ½è±¡æ¦‚æ‹¬â€ï¼Œä¸å¾—æ–°å¢ææ–™å¤–äº‹å®ã€æ¡ä»¶ã€ä¸»ä½“æˆ–ç»“è®ºã€‚

ã€æœ¬æ¬¡ Bloom å¼ºåˆ¶åˆ†é…ã€‘
{bloom_req_text}
"""

    response = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0.7,
    )

    raw = (response.choices[0].message.content or "").strip()
    json_text = extract_json_array(raw)
    if not json_text:
        print("âš  æœªèƒ½ä»æ¨¡å‹è¾“å‡ºä¸­æå– JSON æ•°ç»„ã€‚è¾“å‡ºç‰‡æ®µï¼š", raw[:200], "...")
        return []

    try:
        data = json.loads(json_text)
    except Exception as e:
        print("âš  è§£æ JSON å¤±è´¥ï¼š", e)
        print("åŸå§‹ JSON ç‰‡æ®µï¼š", json_text[:300], "...")
        return []

    if not isinstance(data, list):
        return []

    mcqs: List[Dict] = []
    for i, item in enumerate(data):
        if not isinstance(item, dict):
            continue

        q = str(item.get("question", "")).strip()
        options = item.get("options", [])
        answer = str(item.get("answer", "")).strip()

        bloom_level = str(item.get("bloom_level", "")).strip()
        bloom_label = str(item.get("bloom_label", "")).strip()

        if not q or not isinstance(options, list) or len(options) != 4:
            continue
        options = [str(opt).strip() for opt in options]

        if answer not in ("A", "B", "C", "D"):
            continue

        if bloom_level not in BLOOM_LEVELS:
            continue

        # è‡ªåŠ¨è¡¥å…¨/æ ¡éªŒ bloom_label
        expected_label = BLOOM_LEVELS[bloom_level]
        if bloom_label:
            if bloom_label != expected_label:
                continue
        else:
            bloom_label = expected_label

        # âœ… å¼ºåˆ¶æ£€æŸ¥ bloom æ˜¯å¦åŒ¹é…ç›®æ ‡åˆ†é…
        if i < len(target_blooms):
            if bloom_level != target_blooms[i]:
                continue

        mcqs.append({
            "question": q,
            "options": options,
            "answer": answer,
            "bloom_level": bloom_level,
            "bloom_label": bloom_label,
        })

    # æœ€å¤šè¿”å› n_questions
    return mcqs[:n_questions]


# ========== ä¸»é€»è¾‘ï¼šåˆ†å—ç”Ÿæˆ MCQ + è¿›åº¦æ˜¾ç¤º ==========

def generate_mcq_with_llm(
    nodes: Dict[str, Dict],
    edges: List[Dict],
    sentences: Dict[str, Dict],
) -> List[Dict]:
    """
    åˆ†å—å–‚ factsï¼ˆKG è¾¹ + åŸå¥ï¼‰ï¼Œè°ƒç”¨ LLM ç”Ÿæˆå¤šé“å•é€‰é¢˜ï¼ˆå¸¦ Bloomï¼‰ã€‚
    """
    facts = build_fact_strings(nodes, edges, sentences)
    chunks = chunk_list(facts, EDGES_PER_CHUNK)

    all_mcq_rows: List[Dict] = []
    q_counter = 1
    total_chunks = len(chunks)
    avg_call_time: List[float] = []

    print(f"\nğŸ”„ å…± {total_chunks} ä¸ª fact chunkï¼Œå°†ç”Ÿæˆæœ€å¤š {MAX_QUESTIONS} é“é¢˜ï¼ˆå¸¦ Bloom æ ‡æ³¨ï¼‰\n")

    for chunk_idx, fact_chunk in enumerate(chunks, start=1):
        if len(all_mcq_rows) >= MAX_QUESTIONS:
            break

        remain = MAX_QUESTIONS - len(all_mcq_rows)
        n_q = min(QUESTIONS_PER_CHUNK, remain)

        target_blooms = bloom_targets_for_chunk(chunk_idx, n_q)

        print(f"\nğŸ“Œ Chunk {chunk_idx}/{total_chunks}: å°è¯•ç”Ÿæˆ {n_q} é“é¢˜ | Bloom ç›®æ ‡ï¼š{target_blooms}")
        print("   ğŸ¤– è°ƒç”¨ LLM ä¸­â€¦â€¦")

        start_time = time.time()
        mcqs = call_llm_for_mcq(fact_chunk, target_blooms)
        cost = time.time() - start_time
        avg_call_time.append(cost)

        print(f"   âœ… LLM è¿”å›ï¼ˆè€—æ—¶ {cost:.2f} ç§’ï¼‰ï¼Œé€šè¿‡æ ¡éªŒé¢˜æ•°ï¼š{len(mcqs)}")

        for item in mcqs:
            qid = f"q{q_counter:04d}"
            options = item["options"]
            row = {
                "qid": qid,
                "question": item["question"],
                "option_a": options[0],
                "option_b": options[1],
                "option_c": options[2],
                "option_d": options[3],
                "answer": item["answer"],
                "bloom_level": item.get("bloom_level", ""),
                "bloom_label": item.get("bloom_label", ""),
            }
            all_mcq_rows.append(row)
            q_counter += 1

        # ETA
        progress = len(all_mcq_rows)
        if avg_call_time and progress > 0:
            avg_t = sum(avg_call_time) / len(avg_call_time)
            remain_calls = max((MAX_QUESTIONS - progress) / max(QUESTIONS_PER_CHUNK, 1), 0)
            eta = remain_calls * avg_t
            print(f"   ğŸ“Š è¿›åº¦ï¼šå·²ç”Ÿæˆ {progress}/{MAX_QUESTIONS} é“é¢˜ | é¢„ä¼°å‰©ä½™æ—¶é—´çº¦ï¼š{eta:.1f} ç§’")

    print("\nğŸ‰ é¢˜ç›®ç”Ÿæˆå®Œæˆï¼")
    return all_mcq_rows


# ========== ä¿å­˜ TSV ==========

def save_mcq(rows: List[Dict], path: str):
    """ä¿å­˜ TSVï¼šqid, question, option_a, option_b, option_c, option_d, answer, bloom_level, bloom_label"""
    if not rows:
        print("âš  æ²¡æœ‰ç”Ÿæˆä»»ä½•é¢˜ç›®ï¼Œæ–‡ä»¶ä¸ä¼šå†™å‡ºã€‚")
        return

    os.makedirs(os.path.dirname(path), exist_ok=True)
    fieldnames = [
        "qid",
        "question",
        "option_a",
        "option_b",
        "option_c",
        "option_d",
        "answer",
        "bloom_level",
        "bloom_label",
    ]

    with open(path, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter="\t")
        writer.writeheader()
        for r in rows:
            writer.writerow(r)

    print(f"\nâœ… å·²ä¿å­˜å•é€‰é¢˜ï¼š{path}ï¼ˆå…± {len(rows)} é¢˜ï¼‰")


# ========== main ==========

def main():
    if not os.path.exists(NODES_TSV) or not os.path.exists(EDGES_TSV):
        raise FileNotFoundError("è¯·æ£€æŸ¥èŠ‚ç‚¹/è¾¹ TSV è·¯å¾„æ˜¯å¦æ­£ç¡®")
    if not os.path.exists(SENT_TSV):
        raise FileNotFoundError("è¯·æ£€æŸ¥å¥å­ TSV è·¯å¾„æ˜¯å¦æ­£ç¡®")
    if not os.path.exists(PROMPT_PATH):
        raise FileNotFoundError("è¯·æ£€æŸ¥ prompt è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼ˆPROMPT_PATHï¼‰")

    nodes = load_nodes(NODES_TSV)
    edges = load_edges(EDGES_TSV)
    sentences = load_sentences(SENT_TSV)

    print(f"ğŸ“„ å·²åŠ è½½èŠ‚ç‚¹æ•°ï¼š{len(nodes)}ï¼Œè¾¹æ•°ï¼š{len(edges)}ï¼Œå¥å­æ•°ï¼š{len(sentences)}")

    mcq_rows = generate_mcq_with_llm(nodes, edges, sentences)
    save_mcq(mcq_rows, OUTPUT_Q_TSV)


if __name__ == "__main__":
    main()
