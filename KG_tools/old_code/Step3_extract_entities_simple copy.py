import os
import csv
import re
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
from pipeline_config import STEP2_SENT_TSV, STEP3_ENT_TSV

sent_tsv_path = str(STEP2_SENT_TSV)
output_entity_path = str(STEP3_ENT_TSV)

# ========= é…ç½®åŒºåŸŸï¼šæ”¹æˆä½ è‡ªå·±çš„è·¯å¾„ =========
# sent_tsv_path = r"D:\Desktop\KG_allprocess\KG_files\Output_files\Step2_output\ç¬¬ä¸€è®²_å¥å­åˆ—è¡¨.tsv"
# output_entity_path = r"D:\Desktop\KG_allprocess\KG_files\Output_files\Step3_output\ç¬¬ä¸€è®²_å®ä½“åˆ—è¡¨.tsv"

# NER æ¨¡å‹é…ç½®
model_name = "uer/roberta-base-finetuned-cluener2020-chinese"
device = 0              # CPU=-1ï¼›å¦‚æœæœ‰ GPU å°±å†™ 0
score_threshold = 0.50   # è¿‡æ»¤ä½ç½®ä¿¡åº¦å®ä½“
min_char_len = 2         # å®ä½“æœ€å°é•¿åº¦ï¼ˆåŸºç¡€è¿‡æ»¤ï¼‰

# ä¸­æ–‡æ•°å­— + é˜¿æ‹‰ä¼¯æ•°å­—ï¼Œç”¨äºè¿‡æ»¤â€œçº¯æ•°å­—å®ä½“â€
CH_NUMERIC_CHARS = set("ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶ã€‡0ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™0123456789")


def load_sentences_from_tsv(tsv_path):
    """
    åŠ è½½ Step2 ç”Ÿæˆçš„ TSVï¼šsentence_id | page_no | text
    """
    if not os.path.exists(tsv_path):
        raise FileNotFoundError(tsv_path)

    sentences = []
    with open(tsv_path, "r", encoding="utf-8") as f:
        header = f.readline()  # è·³è¿‡è¡¨å¤´
        for line in f:
            parts = line.rstrip("\n").split("\t")
            if len(parts) < 3:
                continue
            sentence_id, page_no, text = parts[0], parts[1], parts[2]
            sentences.append(
                {
                    "sentence_id": sentence_id,
                    "page_no": int(page_no),
                    "text": text,
                }
            )
    return sentences


def clean_mention(mention: str) -> str:
    """
    æ¸…æ´—å®ä½“å­—ç¬¦ä¸²ï¼š
    - å»æ‰é‡Œé¢æ‰€æœ‰ç©ºç™½å­—ç¬¦
    - å»æ‰å‰åçš„ ã€Šã€‹ã€Œã€ã€ã€‘[]()ï¼ˆï¼‰ ç­‰
    - å»æ‰å¤šä½™çš„å‰å¯¼â€œå’Œâ€â€œçš„â€ï¼ˆå¦‚æœé•¿åº¦è¶³å¤Ÿï¼‰
    - ç‰¹åˆ«å¤„ç†ï¼šåˆ‘æ³•ä¿®æ­£æ¡ˆ(å…« -> åˆ‘æ³•ä¿®æ­£æ¡ˆ(å…«)
    """
    # å»æ‰æ‰€æœ‰ç©ºç™½
    m = re.sub(r"\s+", "", mention)

    # å»æ‰ä¹¦åå·/æ‹¬å·ç­‰å¤–å›´ç¬¦å·
    m = m.strip("ã€Šã€‹ã€Œã€ã€ã€‘[]()ï¼ˆï¼‰\"'â€œâ€â€˜â€™")

    # å»æ‰å‰å¯¼â€œå’Œâ€â€œçš„â€ï¼ˆå¦‚æœå®ä½“å¤Ÿé•¿ï¼Œé¿å…å½±å“è¯¸å¦‚â€œå’Œå¹³æ–¹â€è¿™ç§çœŸå®ä½“ï¼‰
    if len(m) > 3 and m[0] in ("å’Œ", "çš„"):
        m = m[1:]

    # ------- ç‰¹æ®Šå¤„ç†ï¼šåˆ‘æ³•ä¿®æ­£æ¡ˆ(å…« / åˆ‘æ³•ä¿®æ­£æ¡ˆï¼ˆå…« -------
    # 1ï¼‰åŠè§’æ‹¬å·
    if m.startswith("åˆ‘æ³•ä¿®æ­£æ¡ˆ(") and not (m.endswith(")") or m.endswith("ï¼‰")):
        m = m + ")"

    # 2ï¼‰å…¨è§’æ‹¬å·
    if m.startswith("åˆ‘æ³•ä¿®æ­£æ¡ˆï¼ˆ") and not (m.endswith(")") or m.endswith("ï¼‰")):
        m = m + "ï¼‰"

    return m



def is_pure_numeric(mention: str) -> bool:
    """
    åˆ¤æ–­å®ä½“æ˜¯å¦â€œçº¯æ•°å­—/çº¯ä¸­æ–‡æ•°å­—â€ï¼š
    ä¾‹å¦‚ï¼šåäºŒã€ä¸€ã€2010
    """
    if not mention:
        return False
    return all(ch in CH_NUMERIC_CHARS for ch in mention)


def is_bad_mention(mention: str, ent_type: str) -> bool:
    """
    å„ç§â€œåƒåœ¾å®ä½“â€çš„è¿‡æ»¤è§„åˆ™é›†åˆã€‚
    è¿”å› True è¡¨ç¤ºä¸¢å¼ƒã€‚
    """
    # åŸºç¡€é•¿åº¦è¿‡æ»¤
    if len(mention) < min_char_len:
        return True

    # ä¸¢å¼ƒçº¯æ•°å­—å®ä½“ï¼ˆæ¯”å¦‚ â€œåäºŒâ€ã€â€œ2010â€ï¼‰
    if is_pure_numeric(mention):
        return True

    # å¯¹ book ç±»å®ä½“åšæ›´ä¸¥æ ¼ä¸€ç‚¹çš„è¿‡æ»¤
    if ent_type == "book":
        # å¾ˆçŸ­çš„ book å®ä½“ï¼Œä¸€èˆ¬æ²¡ä»€ä¹ˆç”¨
        if len(mention) < 3:
            return True

        # ä»¥â€œçš„ / å’Œâ€å¼€å¤´ä¸”æ•´ä½“å¾ˆçŸ­çš„ï¼Œé€šå¸¸æ˜¯æ®‹ç¼ºç‰‡æ®µï¼Œæ¯”å¦‚â€œçš„å†³å®šâ€â€œå’Œåˆ‘æ³•â€
        if mention[0] in ("çš„", "å’Œ") and len(mention) <= 4:
            return True

        # ç‰¹åˆ«æ’é™¤ä¸€äº›å¾ˆæ³›çš„çŸ­è¯
        if mention in ("å†³å®š", "ä¿®æ­£æ¡ˆ"):
            return True

    return False


def postprocess_entities(raw_entities):
    """
    å¯¹æ¨¡å‹ç›´æ¥è¾“å‡ºçš„å®ä½“åšåå¤„ç†ï¼š
    1. sentence å†…å»æ‰é‡å å®ä½“ï¼ˆä¿ç•™æ›´é•¿ / ç½®ä¿¡åº¦æ›´é«˜çš„ï¼‰
    2. å…¨å±€æŒ‰ (page_no, mention, ent_type) å»é‡
    """
    # 1) å¥å­å†…éƒ¨å¤„ç†ï¼šå»é‡å  + åŒå¥é‡å¤
    ents_by_sent = {}
    for e in raw_entities:
        ents_by_sent.setdefault(e["sentence_id"], []).append(e)

    cleaned = []

    for sent_id, ents in ents_by_sent.items():
        # å…ˆæŒ‰ start_char æ’åºï¼Œé•¿çš„ä¼˜å…ˆ
        ents_sorted = sorted(
            ents,
            key=lambda x: (x["start_char"], -(x["end_char"] - x["start_char"]))
        )

        kept = []
        for e in ents_sorted:
            overlap = False
            for k in kept:
                # åŒç±»å‹ & span é‡å  â†’ è®¤ä¸ºæ˜¯åŒä¸€ç‰‡åŒºåŸŸçš„ç«äº‰å®ä½“
                if e["ent_type"] == k["ent_type"]:
                    if not (e["end_char"] <= k["start_char"] or e["start_char"] >= k["end_char"]):
                        # æœ‰é‡å ï¼Œæ¯”è¾ƒè°æ›´â€œå¥½â€
                        len_e = e["end_char"] - e["start_char"]
                        len_k = k["end_char"] - k["start_char"]
                        if len_e < len_k:
                            overlap = True
                            break
                        elif len_e == len_k and e["confidence"] <= k["confidence"]:
                            overlap = True
                            break
                        else:
                            # å½“å‰çš„æ›´å¥½ï¼Œæ·˜æ±°æ—§çš„
                            kept.remove(k)
                            break
            if not overlap:
                kept.append(e)

        cleaned.extend(kept)

    # 2) å…¨å±€æŒ‰ (page_no, mention, ent_type) å»é‡
    unique = []
    seen = set()
    for e in cleaned:
        key = (e["page_no"], e["mention"], e["ent_type"])
        if key in seen:
            continue
        seen.add(key)
        unique.append(e)

    return unique


def run_ner(sentences):
    """
    ä½¿ç”¨ HuggingFace pipeline æ‰§è¡Œä¸­æ–‡ NERï¼Œå¹¶åšæ¸…æ´— & è¿‡æ»¤ & å»é‡ã€‚
    """
    print(f"ğŸ” æ­£åœ¨åŠ è½½æ¨¡å‹ï¼š{model_name} ...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForTokenClassification.from_pretrained(model_name)

    ner_pipe = pipeline(
        "ner",
        model=model,
        tokenizer=tokenizer,
        aggregation_strategy="simple",  # æ–°å†™æ³•ï¼Œæ›¿ä»£ grouped_entities
        device=device,
    )
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼å¼€å§‹æŠ½å–å®ä½“...\n")

    raw_entities = []
    ent_id = 1

    for s in sentences:
        text = s["text"]
        if not text.strip():
            continue

        results = ner_pipe(text)

        for r in results:
            score = float(r["score"])
            if score < score_threshold:
                continue

            raw_mention = r["word"]
            mention = clean_mention(raw_mention)
            ent_type = r.get("entity_group", "Entity")

            if is_bad_mention(mention, ent_type):
                continue

            raw_entities.append(
                {
                    "entity_id": f"e{ent_id:05d}",
                    "sentence_id": s["sentence_id"],
                    "page_no": s["page_no"],
                    "mention": mention,
                    "start_char": int(r["start"]),
                    "end_char": int(r["end"]),
                    "ent_type": ent_type,
                    "confidence": score,
                }
            )
            ent_id += 1

    print(f"ğŸ§¹ æ¨¡å‹åŸå§‹å®ä½“æ•°ï¼š{len(raw_entities)}ï¼Œå¼€å§‹åšé‡å /é‡å¤è¿‡æ»¤...")
    final_entities = postprocess_entities(raw_entities)
    print(f"âœ… è¿‡æ»¤åå®ä½“æ•°ï¼š{len(final_entities)}")

    return final_entities


def save_entities(entities, output_path):
    """
    è¾“å‡º TSVï¼š
    entity_id | sentence_id | page_no | mention | start_char | end_char | ent_type | confidence
    """
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    with open(output_path, "w", encoding="utf-8", newline="") as f:
        f.write("entity_id\tsentence_id\tpage_no\tmention\tstart_char\tend_char\tent_type\tconfidence\n")
        for e in entities:
            f.write(
                f"{e['entity_id']}\t{e['sentence_id']}\t{e['page_no']}\t"
                f"{e['mention']}\t{e['start_char']}\t{e['end_char']}\t"
                f"{e['ent_type']}\t{e['confidence']}\n"
            )

    print(f"âœ… å®ä½“åˆ—è¡¨å·²ä¿å­˜åˆ°ï¼š{output_path}")
    print(f"ğŸ“Œ å…±æŠ½å–å®ä½“æ•°é‡ï¼š{len(entities)}")


def main():
    sentences = load_sentences_from_tsv(sent_tsv_path)
    print(f"ğŸ“„ å·²åŠ è½½å¥å­æ•°é‡ï¼š{len(sentences)}")

    entities = run_ner(sentences)

    # é¢„è§ˆå‰ 10 æ¡
    print("\nğŸ“Œ å®ä½“ç¤ºä¾‹ï¼ˆå‰10æ¡ï¼‰ï¼š")
    for e in entities[:10]:
        print(
            f"{e['entity_id']}: {e['mention']} "
            f"({e['ent_type']}, p{e['page_no']}, score={e['confidence']:.2f})"
        )

    save_entities(entities, output_entity_path)


if __name__ == "__main__":
    main()
