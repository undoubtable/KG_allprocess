"""
Step8 â€” ä½¿ç”¨ LLM ä»çŸ¥è¯†å›¾è°± + åŸå§‹å¥å­ç”Ÿæˆæ³•å¾‹å•é€‰é¢˜ï¼ˆMCQï¼‰

âœ… v2 æ”¹åŠ¨è¦ç‚¹ï¼ˆå…³é”®ï¼‰ï¼š
1) ç”Ÿæˆæ—¶è¦æ±‚ LLM ä¸ºæ¯é¢˜è¿”å› fact_indexï¼ˆä½¿ç”¨äº†ç¬¬å‡ æ¡äº‹å®ï¼‰
2) å†™å‡º TSV æ—¶æ–°å¢ä¸¤åˆ—ï¼š
   - kg_fact:  src_id|relation_type|dst_id   ï¼ˆå¯ç”¨äºä¸¥æ ¼å®¡è®¡ C/Dï¼‰
   - context:  å¯¹åº” sentence_id çš„åŸå¥ï¼ˆå¯ç”¨äº R1 judge ä¸è¿½æº¯ï¼‰
3) ä»ä¿ç•™ chunk è°ƒç”¨ï¼ˆæ•ˆç‡æ›´é«˜ï¼‰ï¼Œä½†æ¯é¢˜å¯è¿½æº¯åˆ°å…·ä½“ KG edge

è¾“å‡ºå­—æ®µï¼š
qid, question, option_a, option_b, option_c, option_d, answer, kg_fact, context
"""

import csv
import os
import json
import time
from typing import List, Dict, Any, Tuple

from openai import OpenAI
from pipeline_config import STEP4_NODES_TSV, STEP4_EDGES_TSV, STEP2_SENT_TSV
from pipeline_config import STEP8_Q_TSV, PROMPT_PATH

import yaml


# ========== è·¯å¾„é…ç½® ==========
NODES_TSV = str(STEP4_NODES_TSV)
EDGES_TSV = str(STEP4_EDGES_TSV)
SENT_TSV = str(STEP2_SENT_TSV)
OUTPUT_Q_TSV = str(STEP8_Q_TSV)

# ========== LLM é…ç½® ==========
with open("config.yaml", "r", encoding="utf-8") as f:
    config = yaml.safe_load(f)

client = OpenAI(
    base_url="https://ai.gitee.com/v1",
    api_key=config["api_key"],
    default_headers={"X-Failover-Enabled": "true"},
)

# âœ… é¢˜ç›®ç”Ÿæˆæ¨èç”¨ DeepSeek-V3ï¼ˆæˆ– v3.2 æ€è€ƒå…³ï¼‰
MODEL_NAME = config.get("qg_model", "DeepSeek-V3")

# ========== ç”Ÿæˆç­–ç•¥å‚æ•° ==========
MAX_QUESTIONS = 50
EDGES_PER_CHUNK = 5
QUESTIONS_PER_CHUNK = 3

# ========== å·¥å…·å‡½æ•° ==========
def load_prompt_text(path: str) -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read()

SYSTEM_PROMPT = load_prompt_text(PROMPT_PATH)

def load_nodes(path: str) -> Dict[str, Dict[str, str]]:
    nodes: Dict[str, Dict[str, str]] = {}
    with open(path, encoding="utf-8") as f:
        reader = csv.DictReader(f, delimiter="\t")
        for r in reader:
            nodes[r["node_id"]] = r
    return nodes

def load_edges(path: str) -> List[Dict[str, str]]:
    edges: List[Dict[str, str]] = []
    with open(path, encoding="utf-8") as f:
        reader = csv.DictReader(f, delimiter="\t")
        for r in reader:
            edges.append(r)
    return edges

def load_sentences(path: str) -> Dict[str, Dict[str, str]]:
    """
    è¯»å– Step2 çš„å¥å­ TSVï¼š
    å‡è®¾åˆ—é¡ºåºä¸ºï¼šsentence_id | page_no | text
    """
    sentences: Dict[str, Dict[str, str]] = {}
    if not os.path.exists(path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°å¥å­ TSV æ–‡ä»¶ï¼š{path}")

    with open(path, "r", encoding="utf-8") as f:
        _ = f.readline()  # è·³è¿‡è¡¨å¤´
        for line in f:
            line = line.rstrip("\n")
            if not line:
                continue
            parts = line.split("\t")
            if len(parts) < 3:
                continue
            sentence_id = parts[0]
            page_no = parts[1]
            text = parts[2]
            sentences[sentence_id] = {
                "sentence_id": sentence_id,
                "page_no": page_no,
                "text": text,
            }
    return sentences

def chunk_list(lst: List[Any], size: int) -> List[List[Any]]:
    chunks: List[List[Any]] = []
    for i in range(0, len(lst), size):
        chunks.append(lst[i:i + size])
    return chunks

def build_fact_items(
    nodes: Dict[str, Dict[str, str]],
    edges: List[Dict[str, str]],
    sentences: Dict[str, Dict[str, str]],
) -> List[Dict[str, str]]:
    """
    æ¯æ¡ edge æ„é€ æˆä¸€ä¸ª fact_itemï¼š
    - display_text: ç»™ LLM çœ‹çš„äººç±»å¯è¯»äº‹å® + åŸå¥
    - kg_fact: src_id|rel|dst_idï¼ˆç”¨äºä¸¥æ ¼è¯„ä¼°ï¼‰
    - context: åŸå¥
    """
    items: List[Dict[str, str]] = []
    for e in edges:
        src_id = e["src_id"]
        dst_id = e["dst_id"]
        rel = e.get("relation_type", "related_to")

        src_name = nodes.get(src_id, {}).get("name", src_id)
        dst_name = nodes.get(dst_id, {}).get("name", dst_id)

        sent_id = e.get("sentence_id", "")
        sent_text = sentences.get(sent_id, {}).get("text", "").strip()

        display = f"äº‹å®ï¼š{src_name} --{rel}--> {dst_name}"
        if sent_text:
            display += f"\næ¥æºåŸå¥ï¼š{sent_text}"

        items.append({
            "display_text": display,
            "kg_fact": f"{src_id}|{rel}|{dst_id}",
            "context": sent_text,
        })
    return items


# ========== LLM è°ƒç”¨ï¼šç”Ÿæˆ MCQï¼ˆæ¯é¢˜è¿”å› fact_indexï¼‰ ==========
def call_llm_for_mcq(fact_items: List[Dict[str, str]], n_questions: int) -> List[Dict[str, Any]]:
    """
    è¿”å›æ ¼å¼ï¼ˆå¼ºåˆ¶ï¼‰ï¼š
    [
      {
        "fact_index": 1,
        "question": "...",
        "options": ["A. ...","B. ...","C. ...","D. ..."],
        "answer": "B"
      },
      ...
    ]
    å…¶ä¸­ fact_index æŒ‡å‘æœ¬ chunk ä¸­ç¼–å·çš„äº‹å®ï¼ˆä» 1 å¼€å§‹ï¼‰
    """
    if not fact_items or n_questions <= 0:
        return []

    facts_text = "\n".join(
        f"{idx+1}. {it['display_text']}" for idx, it in enumerate(fact_items)
    )

    user_prompt = f"""
ä¸‹é¢æ˜¯è‹¥å¹²æ¡æ¥è‡ªæ³•å¾‹çŸ¥è¯†å›¾è°±çš„â€œäº‹å®åŠå…¶æ¥æºåŸå¥â€ï¼ˆå·²ç¼–å·ï¼‰ï¼š

{facts_text}

è¯·ä½ ã€ä»…æ ¹æ®ä¸Šè¿°äº‹å®åŠåŸå¥ã€‘ç”Ÿæˆ {n_questions} é“ä¸­æ–‡æ³•å¾‹å•é€‰é¢˜ï¼ˆMCQï¼‰ï¼Œå¹¶æ»¡è¶³ï¼š

1) æ¯é“é¢˜å¿…é¡»åŒ…å«å­—æ®µï¼š
   - "fact_index"ï¼šæ•´æ•°ï¼Œè¡¨ç¤ºæœ¬é¢˜ä½¿ç”¨äº†ä¸Šé¢ç¬¬å‡ æ¡äº‹å®ï¼ˆä»1å¼€å§‹ï¼‰
   - "question"ï¼šé¢˜å¹²
   - "options"ï¼šå››ä¸ªå…ƒç´ çš„æ•°ç»„ ["A. ...","B. ...","C. ...","D. ..."]
   - "answer"ï¼šæ­£ç¡®é€‰é¡¹å­—æ¯ï¼ˆA/B/C/Dï¼‰

2) é¢˜å¹²ä¸æ­£ç¡®ç­”æ¡ˆå¿…é¡»èƒ½ä»å¯¹åº”çš„é‚£æ¡äº‹å®æ¨å¯¼å‡ºæ¥ï¼›ä¸å¾—å¼•å…¥ææ–™å¤–ä¿¡æ¯ã€‚
3) é€‰é¡¹éœ€è¦è¿·æƒ‘æ€§ï¼Œä½†ä¸èƒ½æ˜æ˜¾é”™è¯¯æˆ–ä¸åŸå¥çŸ›ç›¾ã€‚
4) æ¯é¢˜åªæœ‰ä¸€ä¸ªæ­£ç¡®ç­”æ¡ˆã€‚
5) åªè¾“å‡º JSON æ•°ç»„ï¼Œä¸è¦è¾“å‡º markdown æˆ–è§£é‡Šã€‚

è¾“å‡ºç¤ºä¾‹ï¼š
[
  {{"fact_index": 2, "question": "...", "options": ["A. ...","B. ...","C. ...","D. ..."], "answer": "B"}}
]
""".strip()

    response = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0.7,
    )

    content = (response.choices[0].message.content or "").strip()

    try:
        # å…¼å®¹ ```json åŒ…è£¹
        if content.startswith("```"):
            content = content.strip("`")
            lb = content.find("[")
            if lb != -1:
                content = content[lb:]
        start = content.find("[")
        end = content.rfind("]")
        if start != -1 and end != -1:
            content = content[start:end+1]

        data = json.loads(content)
        out: List[Dict[str, Any]] = []
        if isinstance(data, list):
            for item in data:
                if not isinstance(item, dict):
                    continue
                fi = item.get("fact_index", None)
                q = str(item.get("question", "")).strip()
                options = item.get("options", [])
                ans = str(item.get("answer", "")).strip().upper()

                if not isinstance(fi, int) or fi < 1 or fi > len(fact_items):
                    continue
                if not q or not isinstance(options, list) or len(options) != 4:
                    continue
                options = [str(x).strip() for x in options]
                if ans not in ("A", "B", "C", "D"):
                    continue

                out.append({
                    "fact_index": fi,
                    "question": q,
                    "options": options,
                    "answer": ans,
                })
        return out[:n_questions]
    except Exception as e:
        print("âš  è§£æ LLM è¾“å‡º JSON å¤±è´¥ï¼š", e)
        print("åŸå§‹å†…å®¹ç‰‡æ®µï¼š", content[:300], "...")
        return []


# ========== ä¸»é€»è¾‘ï¼šåˆ†å—ç”Ÿæˆ + å†™å‡ºå¯å®¡è®¡å­—æ®µ ==========
def generate_mcq_with_llm(
    nodes: Dict[str, Dict[str, str]],
    edges: List[Dict[str, str]],
    sentences: Dict[str, Dict[str, str]],
) -> List[Dict[str, str]]:
    fact_items = build_fact_items(nodes, edges, sentences)
    chunks = chunk_list(fact_items, EDGES_PER_CHUNK)

    all_rows: List[Dict[str, str]] = []
    q_counter = 1

    print(f"\nğŸ”„ å…± {len(chunks)} ä¸ª chunkï¼Œå°†ç”Ÿæˆæœ€å¤š {MAX_QUESTIONS} é“é¢˜\n")

    avg_call_time: List[float] = []
    for chunk_idx, fact_chunk in enumerate(chunks, start=1):
        if len(all_rows) >= MAX_QUESTIONS:
            break

        remain = MAX_QUESTIONS - len(all_rows)
        n_q = min(QUESTIONS_PER_CHUNK, remain)

        print(f"\nğŸ“Œ Chunk {chunk_idx}/{len(chunks)}ï¼šå°è¯•ç”Ÿæˆ {n_q} é“é¢˜")
        start_time = time.time()
        mcqs = call_llm_for_mcq(fact_chunk, n_q)
        cost = time.time() - start_time
        avg_call_time.append(cost)
        print(f"   âœ… è¿”å› {len(mcqs)} é“ï¼ˆè€—æ—¶ {cost:.2f}sï¼‰")

        for item in mcqs:
            qid = f"q{q_counter:04d}"
            opts = item["options"]
            fi = item["fact_index"] - 1  # 0-based
            kg_fact = fact_chunk[fi]["kg_fact"]
            context = fact_chunk[fi]["context"]

            row = {
                "qid": qid,
                "question": item["question"],
                "option_a": opts[0],
                "option_b": opts[1],
                "option_c": opts[2],
                "option_d": opts[3],
                "answer": item["answer"],
                "kg_fact": kg_fact,
                "context": context,
            }
            all_rows.append(row)
            q_counter += 1

        # ETA
        if avg_call_time and len(all_rows) > 0:
            avg_t = sum(avg_call_time) / len(avg_call_time)
            remain_calls = max((MAX_QUESTIONS - len(all_rows)) / max(QUESTIONS_PER_CHUNK, 1), 0)
            eta = remain_calls * avg_t
            print(f"   ğŸ“Š è¿›åº¦ï¼š{len(all_rows)}/{MAX_QUESTIONS} | ETAâ‰ˆ{eta:.1f}s")

    print("\nğŸ‰ é¢˜ç›®ç”Ÿæˆå®Œæˆï¼")
    return all_rows


def save_mcq(rows: List[Dict[str, str]], path: str):
    if not rows:
        print("âš  æ²¡æœ‰ç”Ÿæˆä»»ä½•é¢˜ç›®ï¼Œæ–‡ä»¶ä¸ä¼šå†™å‡ºã€‚")
        return

    os.makedirs(os.path.dirname(path), exist_ok=True)
    fieldnames = [
        "qid", "question",
        "option_a", "option_b", "option_c", "option_d",
        "answer",
        "kg_fact",     # âœ… æ–°å¢
        "context",     # âœ… æ–°å¢
    ]

    with open(path, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter="\t")
        writer.writeheader()
        for r in rows:
            writer.writerow(r)

    print(f"\nâœ… å·²ä¿å­˜å•é€‰é¢˜ï¼š{path}ï¼ˆå…± {len(rows)} é¢˜ï¼‰")


def main():
    if not os.path.exists(NODES_TSV) or not os.path.exists(EDGES_TSV):
        raise FileNotFoundError("è¯·æ£€æŸ¥èŠ‚ç‚¹/è¾¹ TSV è·¯å¾„æ˜¯å¦æ­£ç¡®")
    if not os.path.exists(SENT_TSV):
        raise FileNotFoundError("è¯·æ£€æŸ¥å¥å­ TSV è·¯å¾„æ˜¯å¦æ­£ç¡®")

    nodes = load_nodes(NODES_TSV)
    edges = load_edges(EDGES_TSV)
    sentences = load_sentences(SENT_TSV)

    print(f"ğŸ“„ å·²åŠ è½½èŠ‚ç‚¹æ•°ï¼š{len(nodes)}ï¼Œè¾¹æ•°ï¼š{len(edges)}ï¼Œå¥å­æ•°ï¼š{len(sentences)}")
    mcq_rows = generate_mcq_with_llm(nodes, edges, sentences)
    save_mcq(mcq_rows, OUTPUT_Q_TSV)


if __name__ == "__main__":
    main()
