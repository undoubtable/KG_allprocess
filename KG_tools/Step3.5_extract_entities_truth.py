import os
import json
import yaml
import csv
import re
import time
from typing import List, Dict, Any, Optional, Tuple

from openai import OpenAI
from pipeline_config import STEP2_SENT_TSV, STEP35_TRUTH_ENT_TSV

# ===================== è·¯å¾„é…ç½® =====================
sent_tsv_path = str(STEP2_SENT_TSV)
truth_entity_tsv_path = str(STEP35_TRUTH_ENT_TSV)

# ===================== LLM é…ç½® =====================
with open("config.yaml", "r", encoding="utf-8") as f:
    config = yaml.safe_load(f)

client = OpenAI(
    base_url="https://ai.gitee.com/v1",
    api_key=config["api_key"],
    default_headers={"X-Failover-Enabled": "true"},
)

CANDIDATE_MODEL = "DeepSeek-V3"
VERIFY_MODEL = "DeepSeek-R1"  # è‹¥æƒ³æ›´å¿«å…ˆè·‘é€šï¼šå¯ä»¥æ”¹æˆ DeepSeek-V3

# ===================== å‚æ•°æ§åˆ¶ï¼ˆæ¨èè¿™æ ·è®¾ï¼‰ =====================
BATCH_SIZE = 10               # âœ… ä¸è¦ 50ï¼Œå®¹æ˜“è¶…æ—¶
MAX_CAND_PER_SENT = 12
MAX_TRUTH_PER_SENT = 6
DEFAULT_CONF_TRUTH = 0.95

# å€™é€‰å¤ªå°‘å°±ä¸èµ° R1ï¼ˆçœæ—¶é—´ï¼‰
ENABLE_SKIP_VERIFY = True
SKIP_VERIFY_IF_CAND_LEQ = 1

# æ‰“å°æ§åˆ¶ï¼šåªæ‰“å°è¿›åº¦ï¼Œä¸æ‰“å°æ ·ä¾‹
PRINT_EVERY_N_BATCH = 1

# è¯·æ±‚é‡è¯•
MAX_RETRIES = 3
RETRY_BASE_SLEEP = 1.2

# ===================== æ¸…æ´—/è¿‡æ»¤é…ç½® =====================
LIST_SEPS = ("ã€", "ï¼Œ", ",", "ï¼›", ";", "/", "ï¼")
CH_NUMERIC_CHARS = set("ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶ã€‡0ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™0123456789")
BAD_GENERIC = {
    "è¡Œä¸º","è§„å®š","æƒ…å†µ","æ–¹é¢","é—®é¢˜","è¿‡ç¨‹","å†…å®¹","æ–¹å¼","ç»“æœ","å› ç´ ","åŸåˆ™","è¦æ±‚",
    "å¯¹è±¡","è´£ä»»","åˆ¶åº¦","æ ‡å‡†","æªæ–½","æƒ…å½¢","ç›®çš„","æ€§è´¨","æ¦‚å¿µ","å…³ç³»","ä¾æ®","æ¡ä»¶",
    "èŒƒå›´","ç¨‹åº¦","æ–¹æ³•","æ„è§","å†³å®š","é€šçŸ¥","å…¬å‘Š","ææ–™","è¯æ®","äº‹å®","ç†ç”±","ç»“è®º",
}
BAD_SUFFIX = ("æ–¹é¢","é—®é¢˜","æƒ…å†µ","è¿‡ç¨‹","å†…å®¹","æ–¹å¼","ç»“æœ","å› ç´ ","åŸåˆ™","è¦æ±‚","åˆ¶åº¦","å…³ç³»","æ ‡å‡†","æªæ–½","æƒ…å½¢")
MAX_MENTION_LEN = 20
MIN_LEN = 2


def chat_with_retry(model: str, messages: List[Dict[str, str]], temperature: float = 0.1) -> str:
    last_err = None
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            resp = client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
            )
            return (resp.choices[0].message.content or "").strip()
        except Exception as e:
            last_err = e
            sleep_s = RETRY_BASE_SLEEP * attempt
            print(f"âš ï¸ LLMè¯·æ±‚å¤±è´¥ï¼ˆ{model}ï¼‰attempt {attempt}/{MAX_RETRIES}: {e} -> sleep {sleep_s:.1f}s")
            time.sleep(sleep_s)
    raise RuntimeError(f"LLM è¯·æ±‚å¤±è´¥ï¼ˆé‡è¯•ä»å¤±è´¥ï¼‰ï¼š{last_err}")


def load_sentences(tsv_path: str) -> List[Dict[str, Any]]:
    sents = []
    with open(tsv_path, "r", encoding="utf-8") as f:
        _ = f.readline()
        for line in f:
            parts = line.rstrip("\n").split("\t")
            if len(parts) < 3:
                continue
            sid, page_no, text = parts[0], int(parts[1]), parts[2]
            sents.append({"sentence_id": sid, "page_no": page_no, "text": text})
    return sents


def extract_json(content: str) -> Optional[Dict[str, Any]]:
    content = (content or "").strip()
    l = content.find("{")
    r = content.rfind("}")
    if l == -1 or r == -1 or r <= l:
        return None
    try:
        return json.loads(content[l:r + 1])
    except Exception:
        return None


def clean_mention(m: str) -> str:
    m = re.sub(r"\s+", "", m).strip()
    m = m.strip("ã€Šã€‹ã€Œã€ã€ã€‘[]()ï¼ˆï¼‰\"'â€œâ€â€˜â€™")
    if len(m) > 3 and m[0] in ("å’Œ", "çš„"):
        m = m[1:]
    if m.startswith("åˆ‘æ³•ä¿®æ­£æ¡ˆ(") and not (m.endswith(")") or m.endswith("ï¼‰")):
        m += ")"
    if m.startswith("åˆ‘æ³•ä¿®æ­£æ¡ˆï¼ˆ") and not (m.endswith(")") or m.endswith("ï¼‰")):
        m += "ï¼‰"
    return m


def is_pure_numeric(m: str) -> bool:
    return bool(m) and all(ch in CH_NUMERIC_CHARS for ch in m)


def is_bad(m: str) -> bool:
    if len(m) < MIN_LEN or len(m) > MAX_MENTION_LEN:
        return True
    if is_pure_numeric(m):
        return True
    if m in BAD_GENERIC:
        return True
    if m.endswith(BAD_SUFFIX) and len(m) <= 6:
        return True
    if not re.search(r"[\u4e00-\u9fffA-Za-z0-9]", m):
        return True
    return False


def split_list(m: str) -> List[str]:
    if not any(sep in m for sep in LIST_SEPS):
        return [m]
    parts = [m]
    for sep in LIST_SEPS:
        new_parts = []
        for p in parts:
            new_parts.extend(p.split(sep))
        parts = new_parts
    parts = [clean_mention(p) for p in parts if p]
    parts = [p for p in parts if p and not is_bad(p)]
    # å»é‡ä¿åº
    seen = set()
    out = []
    for p in parts:
        if p not in seen:
            seen.add(p)
            out.append(p)
    return out if out else [m]


def find_span(text: str, m: str) -> Optional[Tuple[int, int]]:
    start = text.find(m)
    if start == -1:
        return None
    return start, start + len(m)


def load_done_sentence_ids(path: str) -> set:
    """
    æ–­ç‚¹ç»­è·‘ï¼šè¯»å–å·²å†™å…¥çš„ truth_entity_tsvï¼Œè¿”å›å·²å¤„ç†çš„ sentence_id é›†åˆ
    """
    if not os.path.exists(path):
        return set()
    done = set()
    with open(path, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f, delimiter="\t")
        for row in reader:
            sid = row.get("sentence_id")
            if sid:
                done.add(sid)
    return done


def append_entities_tsv(rows: List[Dict[str, Any]], path: str) -> None:
    """
    è¿½åŠ å†™å…¥ TSVï¼ˆè‹¥æ–‡ä»¶ä¸å­˜åœ¨åˆ™å†™ headerï¼‰
    """
    need_header = not os.path.exists(path)
    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)

    with open(path, "a", encoding="utf-8", newline="") as f:
        if need_header:
            f.write("entity_id\tsentence_id\tpage_no\tmention\tstart_char\tend_char\tent_type\tconfidence\n")
        for r in rows:
            f.write(
                f"{r['entity_id']}\t{r['sentence_id']}\t{r['page_no']}\t{r['mention']}\t"
                f"{r['start_char']}\t{r['end_char']}\t{r['ent_type']}\t{r['confidence']:.4f}\n"
            )


def llm_candidate_entities_batch(batch: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, str]]]:
    system = (
        "ä½ æ˜¯ä¸­æ–‡ä¿¡æ¯æŠ½å–åŠ©æ‰‹ã€‚è¯·å¯¹å¤šä¸ªå¥å­åˆ†åˆ«æŠ½å–å®ä½“ã€‚\n"
        "è¾“å‡ºå¿…é¡»æ˜¯ä¸¥æ ¼ JSONï¼ˆä¸è¦ Markdownï¼Œä¸è¦è§£é‡Šï¼‰ï¼Œæ ¼å¼ï¼š\n"
        "{\"items\":[{\"sentence_id\":\"...\",\"entities\":[{\"mention\":\"...\",\"ent_type\":\"...\"}, ...]}, ...]}\n"
        "è§„åˆ™ï¼š\n"
        "1) mention å¿…é¡»æ˜¯å¯¹åº”å¥å­ä¸­çš„è¿ç»­å­ä¸²ï¼ŒåŸæ–‡å¤åˆ¶ã€‚\n"
        "2) ent_type åªèƒ½é€‰ï¼šPerson, Org, Law, Crime, Location, Time, Book, Concept, Otherã€‚\n"
        f"3) æ¯ä¸ªå¥å­æœ€å¤šè¾“å‡º {MAX_CAND_PER_SENT} ä¸ªå®ä½“ï¼ŒæŒ‰é‡è¦æ€§é™åºã€‚\n"
    )
    user = {"sentences": [{"sentence_id": x["sentence_id"], "text": x["text"]} for x in batch]}
    content = chat_with_retry(
        model=CANDIDATE_MODEL,
        messages=[
            {"role": "system", "content": system},
            {"role": "user", "content": json.dumps(user, ensure_ascii=False)},
        ],
        temperature=0.1,
    )
    data = extract_json(content) or {}
    items = data.get("items", [])
    out: Dict[str, List[Dict[str, str]]] = {}
    if not isinstance(items, list):
        return out

    for it in items:
        if not isinstance(it, dict):
            continue
        sid = str(it.get("sentence_id", "")).strip()
        ents = it.get("entities", [])
        if not sid or not isinstance(ents, list):
            continue
        res = []
        for e in ents:
            if isinstance(e, dict) and e.get("mention"):
                res.append({
                    "mention": str(e["mention"]).strip(),
                    "ent_type": str(e.get("ent_type", "Other")).strip() or "Other"
                })
        out[sid] = res[:MAX_CAND_PER_SENT]
    return out


def llm_verify_entities_batch(batch: List[Dict[str, Any]], cand_map: Dict[str, List[str]]) -> Dict[str, List[Dict[str, str]]]:
    system = (
        "ä½ æ˜¯å®ä½“ç­›é€‰å™¨ã€‚è¯·å¯¹å¤šä¸ªå¥å­åˆ†åˆ«ä»å€™é€‰åˆ—è¡¨ä¸­ç­›é€‰åº”ä¿ç•™çš„å®ä½“ã€‚\n"
        "è¾“å‡ºå¿…é¡»æ˜¯ä¸¥æ ¼ JSONï¼ˆä¸è¦ Markdownï¼Œä¸è¦è§£é‡Šï¼‰ï¼Œæ ¼å¼ï¼š\n"
        "{\"items\":[{\"sentence_id\":\"...\",\"entities\":[{\"mention\":\"...\",\"ent_type\":\"...\"}, ...]}, ...]}\n"
        "ç¡¬è§„åˆ™ï¼š\n"
        "1) mention å¿…é¡»å®Œå…¨æ¥è‡ªè¯¥å¥çš„å€™é€‰å®ä½“åˆ—è¡¨ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰ã€‚\n"
        "2) mention å¿…é¡»æ˜¯è¯¥å¥åŸæ–‡è¿ç»­å­ä¸²ã€‚\n"
        "3) ä¸è¦ä¿ç•™æ³›åŒ–åè¯ï¼ˆè¡Œä¸º/è§„å®š/æƒ…å†µ/æ–¹å¼/ç»“æœ/å› ç´ /åŸåˆ™/åˆ¶åº¦/å…³ç³»ç­‰ï¼‰ã€‚\n"
        f"4) æ¯å¥æœ€å¤šä¿ç•™ {MAX_TRUTH_PER_SENT} ä¸ªï¼ŒæŒ‰é‡è¦æ€§é™åºã€‚\n"
    )
    payload = {
        "items": [
            {"sentence_id": x["sentence_id"], "text": x["text"], "candidates": cand_map.get(x["sentence_id"], [])}
            for x in batch
        ]
    }
    content = chat_with_retry(
        model=VERIFY_MODEL,
        messages=[
            {"role": "system", "content": system},
            {"role": "user", "content": json.dumps(payload, ensure_ascii=False)},
        ],
        temperature=0.1,
    )

    data = extract_json(content) or {}
    items = data.get("items", [])
    out: Dict[str, List[Dict[str, str]]] = {}
    if not isinstance(items, list):
        return out

    for it in items:
        if not isinstance(it, dict):
            continue
        sid = str(it.get("sentence_id", "")).strip()
        ents = it.get("entities", [])
        if not sid or not isinstance(ents, list):
            continue
        res = []
        for e in ents:
            if isinstance(e, dict) and e.get("mention"):
                res.append({
                    "mention": str(e["mention"]).strip(),
                    "ent_type": str(e.get("ent_type", "Other")).strip() or "Other"
                })
        out[sid] = res[:MAX_TRUTH_PER_SENT]
    return out


def main():
    print("========== Truth-Entityï¼ˆbatch + checkpoint + no examplesï¼‰==========")
    sents = load_sentences(sent_tsv_path)
    print(f"ğŸ“„ å·²åŠ è½½å¥å­æ•°ï¼š{len(sents)}")
    print(f"ğŸ¤– å€™é€‰æ¨¡å‹ï¼š{CANDIDATE_MODEL} | ç­›é€‰æ¨¡å‹ï¼š{VERIFY_MODEL}")
    print(f"âš™ï¸ BATCH_SIZE={BATCH_SIZE}, MAX_CAND_PER_SENT={MAX_CAND_PER_SENT}, MAX_TRUTH_PER_SENT={MAX_TRUTH_PER_SENT}")
    if ENABLE_SKIP_VERIFY:
        print(f"âš¡ cand<= {SKIP_VERIFY_IF_CAND_LEQ} æ—¶è·³è¿‡ R1")
    print(f"ğŸ’¾ checkpoint è¾“å‡ºï¼š{truth_entity_tsv_path}\n")

    done_sids = load_done_sentence_ids(truth_entity_tsv_path)
    if done_sids:
        print(f"ğŸ” æ£€æµ‹åˆ°å·²æœ‰è¾“å‡ºæ–‡ä»¶ï¼Œå·²å®Œæˆ sentence æ•°ï¼š{len(done_sids)}ï¼ˆå°†è‡ªåŠ¨è·³è¿‡ï¼‰")

    # è¿‡æ»¤æ‰å·²å®Œæˆçš„å¥å­
    todo = [x for x in sents if x["sentence_id"] not in done_sids]
    print(f"ğŸ§© å¾…å¤„ç†å¥å­æ•°ï¼š{len(todo)}\n")
    if not todo:
        print("âœ… æ²¡æœ‰å¾…å¤„ç†å†…å®¹ï¼Œç»“æŸã€‚")
        return

    total_batches = (len(todo) + BATCH_SIZE - 1) // BATCH_SIZE
    global_eid = 1

    # å¦‚æœå·²æœ‰æ–‡ä»¶ï¼Œä¸ºé¿å… entity_id é‡å¤ï¼Œç®€å•åšæ³•ï¼šä»å·²æœ‰è¡Œæ•°æ¨ eid
    if os.path.exists(truth_entity_tsv_path):
        with open(truth_entity_tsv_path, "r", encoding="utf-8") as f:
            n = sum(1 for _ in f) - 1
        global_eid = max(1, n + 1)

    for bi in range(total_batches):
        batch = todo[bi * BATCH_SIZE: (bi + 1) * BATCH_SIZE]

        # 1) å€™é€‰ï¼ˆV3ï¼‰
        cand_items = llm_candidate_entities_batch(batch)

        # 2) æ¸…æ´—å€™é€‰ï¼ˆè§„åˆ™å±‚ï¼‰
        cand_mentions_map: Dict[str, List[str]] = {}
        for x in batch:
            sid = x["sentence_id"]
            text = x["text"]
            cand = cand_items.get(sid, [])

            cm = []
            for c in cand:
                m0 = clean_mention(c["mention"])
                for m in split_list(m0):
                    if is_bad(m):
                        continue
                    if find_span(text, m) is None:
                        continue
                    if m not in cm:
                        cm.append(m)
            cand_mentions_map[sid] = cm

        # 3) ç­›é€‰ï¼ˆR1ï¼‰æˆ–è·³è¿‡
        need_verify = []
        keep_map: Dict[str, List[Dict[str, str]]] = {}

        if ENABLE_SKIP_VERIFY:
            for x in batch:
                sid = x["sentence_id"]
                cm = cand_mentions_map.get(sid, [])
                if len(cm) <= SKIP_VERIFY_IF_CAND_LEQ:
                    keep_map[sid] = [{"mention": m, "ent_type": "Other"} for m in cm]
                else:
                    need_verify.append(x)
        else:
            need_verify = batch

        if need_verify:
            verify_in = {x["sentence_id"]: cand_mentions_map.get(x["sentence_id"], []) for x in need_verify}
            verified = llm_verify_entities_batch(need_verify, verify_in)
            for sid, ents in verified.items():
                keep_map[sid] = ents

        # 4) ç”Ÿæˆ rows å¹¶è¿½åŠ å†™å…¥ checkpoint
        rows = []
        for x in batch:
            sid = x["sentence_id"]
            text = x["text"]
            page_no = x["page_no"]
            keep = keep_map.get(sid, [])

            for k in keep:
                m = clean_mention(k["mention"])
                sp = find_span(text, m)
                if sp is None:
                    continue
                ent_type = str(k.get("ent_type", "Other") or "Other")
                rows.append({
                    "entity_id": f"e{global_eid:05d}",
                    "sentence_id": sid,
                    "page_no": page_no,
                    "mention": m,
                    "start_char": sp[0],
                    "end_char": sp[1],
                    "ent_type": ent_type,
                    "confidence": DEFAULT_CONF_TRUTH,
                })
                global_eid += 1

        append_entities_tsv(rows, truth_entity_tsv_path)

        if (bi + 1) % PRINT_EVERY_N_BATCH == 0:
            print(f"â€¦batch {bi+1}/{total_batches} å®Œæˆ | æœ¬æ‰¹å†™å…¥ {len(rows)} è¡Œ | å·²è¾“å‡ºæ–‡ä»¶ç´¯è®¡æ›´æ–°")

    print("\nâœ… å…¨éƒ¨å®Œæˆï¼ˆå·²æŒç»­å†™å…¥æ–‡ä»¶ï¼Œä¸æ€•ä¸­æ–­ï¼‰ã€‚")


if __name__ == "__main__":
    main()
