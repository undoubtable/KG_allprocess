import os
import json
import yaml
import csv
import re
from typing import List, Dict, Any, Optional, Tuple
from collections import defaultdict

from openai import OpenAI

# ä½ å¯ä»¥åœ¨ pipeline_config é‡ŒåŠ  truth è¾“å‡ºè·¯å¾„ï¼›æ²¡æœ‰çš„è¯ç”¨æ‰‹åŠ¨è·¯å¾„
from pipeline_config import STEP2_SENT_TSV,STEP35_TRUTH_ENT_TSV

# ========= è·¯å¾„é…ç½® =========
sent_tsv_path = str(STEP2_SENT_TSV)

# æ‰‹åŠ¨æŒ‡å®š truth è¾“å‡ºï¼ˆå»ºè®®ä½ æ”¾åˆ° Step3_truth_output ç›®å½•ï¼‰
truth_entity_tsv_path = str(STEP35_TRUTH_ENT_TSV)

# ========= LLM é…ç½® =========
with open("config.yaml", "r", encoding="utf-8") as f:
    config = yaml.safe_load(f)

client = OpenAI(
    base_url="https://ai.gitee.com/v1",
    api_key=config["api_key"],
    default_headers={"X-Failover-Enabled": "true"},
)

CANDIDATE_MODEL = "DeepSeek-V3"
VERIFY_MODEL = "DeepSeek-R1"

# ========= æ§åˆ¶å‚æ•° =========
MAX_CAND_PER_SENT = 12   # å€™é€‰å¯ä»¥ç¨å¤š
MAX_TRUTH_PER_SENT = 6   # truth æ›´ä¸¥æ ¼æ›´å°‘
DEFAULT_CONF = 0.95      # truth é»˜è®¤æ›´é«˜ï¼ˆä¹Ÿå¯ä»¥åç»­ç”¨ salience æ›¿ä»£ï¼‰

LIST_SEPS = ("ã€", "ï¼Œ", ",", "ï¼›", ";", "/", "ï¼")
CH_NUMERIC_CHARS = set("ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶ã€‡0ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™0123456789")
BAD_GENERIC = {
    "è¡Œä¸º","è§„å®š","æƒ…å†µ","æ–¹é¢","é—®é¢˜","è¿‡ç¨‹","å†…å®¹","æ–¹å¼","ç»“æœ","å› ç´ ","åŸåˆ™","è¦æ±‚",
    "å¯¹è±¡","è´£ä»»","åˆ¶åº¦","æ ‡å‡†","æªæ–½","æƒ…å½¢","ç›®çš„","æ€§è´¨","æ¦‚å¿µ","å…³ç³»","ä¾æ®","æ¡ä»¶",
    "èŒƒå›´","ç¨‹åº¦","æ–¹æ³•","æ„è§","å†³å®š","é€šçŸ¥","å…¬å‘Š","ææ–™","è¯æ®","äº‹å®","ç†ç”±","ç»“è®º",
}
BAD_SUFFIX = ("æ–¹é¢","é—®é¢˜","æƒ…å†µ","è¿‡ç¨‹","å†…å®¹","æ–¹å¼","ç»“æœ","å› ç´ ","åŸåˆ™","è¦æ±‚","åˆ¶åº¦","å…³ç³»","æ ‡å‡†","æªæ–½","æƒ…å½¢")
MAX_MENTION_LEN = 20
MIN_LEN = 2


def load_sentences(tsv_path: str) -> List[Dict[str, Any]]:
    sents = []
    with open(tsv_path, "r", encoding="utf-8") as f:
        _ = f.readline()
        for line in f:
            parts = line.rstrip("\n").split("\t")
            if len(parts) < 3:
                continue
            sid, page_no, text = parts[0], int(parts[1]), parts[2]
            sents.append({"sentence_id": sid, "page_no": page_no, "text": text})
    return sents


def clean_mention(m: str) -> str:
    m = re.sub(r"\s+", "", m).strip()
    m = m.strip("ã€Šã€‹ã€Œã€ã€ã€‘[]()ï¼ˆï¼‰\"'â€œâ€â€˜â€™")
    if len(m) > 3 and m[0] in ("å’Œ", "çš„"):
        m = m[1:]
    if m.startswith("åˆ‘æ³•ä¿®æ­£æ¡ˆ(") and not (m.endswith(")") or m.endswith("ï¼‰")):
        m += ")"
    if m.startswith("åˆ‘æ³•ä¿®æ­£æ¡ˆï¼ˆ") and not (m.endswith(")") or m.endswith("ï¼‰")):
        m += "ï¼‰"
    return m


def is_pure_numeric(m: str) -> bool:
    return bool(m) and all(ch in CH_NUMERIC_CHARS for ch in m)


def is_bad(m: str) -> bool:
    if len(m) < MIN_LEN or len(m) > MAX_MENTION_LEN:
        return True
    if is_pure_numeric(m):
        return True
    if m in BAD_GENERIC:
        return True
    if m.endswith(BAD_SUFFIX) and len(m) <= 6:
        return True
    if not re.search(r"[\u4e00-\u9fffA-Za-z0-9]", m):
        return True
    return False


def split_list(m: str) -> List[str]:
    if not any(sep in m for sep in LIST_SEPS):
        return [m]
    parts = [m]
    for sep in LIST_SEPS:
        new_parts = []
        for p in parts:
            new_parts.extend(p.split(sep))
        parts = new_parts
    parts = [clean_mention(p) for p in parts if p]
    parts = [p for p in parts if not is_bad(p)]
    # å»é‡ä¿åº
    seen = set()
    out = []
    for p in parts:
        if p not in seen:
            seen.add(p)
            out.append(p)
    return out if out else [m]


def find_span(text: str, m: str) -> Optional[Tuple[int, int]]:
    start = text.find(m)
    if start == -1:
        return None
    return start, start + len(m)


def extract_json(content: str) -> Optional[Dict[str, Any]]:
    l = content.find("{")
    r = content.rfind("}")
    if l == -1 or r == -1 or r <= l:
        return None
    try:
        return json.loads(content[l:r + 1])
    except Exception:
        return None


def llm_candidate_entities(text: str) -> List[Dict[str, str]]:
    system = (
        "ä»å¥å­ä¸­æŠ½å–å®ä½“ã€‚è¾“å‡ºä¸¥æ ¼ JSONï¼š"
        "{\"entities\":[{\"mention\":\"...\",\"ent_type\":\"...\"},...]}\n"
        "mention å¿…é¡»æ˜¯åŸå¥è¿ç»­å­ä¸²ï¼›ä¸å¾—æ”¹å†™ã€‚\n"
        "ent_type åªèƒ½é€‰ï¼šPerson, Org, Law, Crime, Location, Time, Book, Concept, Otherã€‚\n"
        f"æœ€å¤šè¾“å‡º {MAX_CAND_PER_SENT} ä¸ªï¼ŒæŒ‰é‡è¦æ€§é™åºã€‚å®å¯å°‘æŠ½ï¼Œä¸è¦èƒ¡ç¼–ã€‚"
    )
    resp = client.chat.completions.create(
        model=CANDIDATE_MODEL,
        messages=[{"role": "system", "content": system},
                  {"role": "user", "content": f"å¥å­ï¼š{text}"}],
        temperature=0.1,
    )
    data = extract_json((resp.choices[0].message.content or "").strip())
    ents = (data or {}).get("entities", [])
    if not isinstance(ents, list):
        return []
    out = []
    for e in ents:
        if isinstance(e, dict) and e.get("mention"):
            out.append({
                "mention": str(e["mention"]).strip(),
                "ent_type": str(e.get("ent_type", "Other")).strip() or "Other"
            })
    return out[:MAX_CAND_PER_SENT]


def llm_verify_entities(text: str, candidates: List[Dict[str, str]]) -> List[Dict[str, str]]:
    """
    R1 ä¸¥æ ¼ç­›é€‰ï¼šè¾“å‡ºå¿…é¡»æ˜¯ candidates çš„å­é›†
    """
    cand_mentions = [c["mention"] for c in candidates]
    system = (
        "ä½ æ˜¯å®ä½“ç­›é€‰å™¨ã€‚ç»™å®šå¥å­ä¸å€™é€‰å®ä½“åˆ—è¡¨ï¼Œè¯·ç­›é€‰å‡ºåº”ä¿ç•™çš„å®ä½“ã€‚\n"
        "è¾“å‡ºä¸¥æ ¼ JSONï¼š{\"entities\":[{\"mention\":\"...\",\"ent_type\":\"...\"},...]}\n"
        "ç¡¬è§„åˆ™ï¼š\n"
        "1) mention å¿…é¡»å®Œå…¨æ¥è‡ªå€™é€‰åˆ—è¡¨ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰ã€‚\n"
        "2) mention å¿…é¡»æ˜¯åŸå¥è¿ç»­å­ä¸²ã€‚\n"
        "3) ä¸è¦æ³›åŒ–åè¯ï¼ˆè¡Œä¸º/è§„å®š/æƒ…å†µ/æ–¹å¼/ç»“æœ/å› ç´ /åŸåˆ™/åˆ¶åº¦/å…³ç³»ç­‰ï¼‰ã€‚\n"
        f"4) æœ€å¤šä¿ç•™ {MAX_TRUTH_PER_SENT} ä¸ªï¼ŒæŒ‰é‡è¦æ€§é™åºã€‚\n"
        "å®å¯å°‘ä¿ç•™ï¼Œä¹Ÿä¸è¦ä¿ç•™ä¸ç¡®å®šçš„ã€‚"
    )
    resp = client.chat.completions.create(
        model=VERIFY_MODEL,
        messages=[
            {"role": "system", "content": system},
            {"role": "user", "content": f"å¥å­ï¼š{text}\nå€™é€‰å®ä½“ï¼š{cand_mentions}"},
        ],
        temperature=0.1,
    )
    data = extract_json((resp.choices[0].message.content or "").strip())
    ents = (data or {}).get("entities", [])
    if not isinstance(ents, list):
        return []

    # å¼ºçº¦æŸï¼šå¿…é¡»æ¥è‡ªå€™é€‰ + å¿…é¡»å­ä¸²
    cand_set = set(cand_mentions)
    out = []
    for e in ents:
        if not isinstance(e, dict):
            continue
        m = str(e.get("mention", "")).strip()
        if m in cand_set and (text.find(m) != -1):
            out.append({"mention": m, "ent_type": str(e.get("ent_type", "Other")).strip() or "Other"})
    # å»é‡ä¿åº + æˆªæ–­
    seen = set()
    dedup = []
    for x in out:
        if x["mention"] not in seen:
            seen.add(x["mention"])
            dedup.append(x)
    return dedup[:MAX_TRUTH_PER_SENT]


def save_tsv(rows: List[Dict[str, Any]], path: str) -> None:
    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
    with open(path, "w", encoding="utf-8", newline="") as f:
        f.write("entity_id\tsentence_id\tpage_no\tmention\tstart_char\tend_char\tent_type\tconfidence\n")
        for r in rows:
            f.write(
                f"{r['entity_id']}\t{r['sentence_id']}\t{r['page_no']}\t{r['mention']}\t"
                f"{r['start_char']}\t{r['end_char']}\t{r['ent_type']}\t{r['confidence']:.4f}\n"
            )
    print(f"âœ… Truth å®ä½“å·²ä¿å­˜ï¼š{path}ï¼ˆ{len(rows)} è¡Œï¼‰")


def main():
    sents = load_sentences(sent_tsv_path)
    print(f"ğŸ“„ sentences: {len(sents)}")

    out = []
    eid = 1
    seen_global = set()  # (page_no, mention, ent_type)

    for i, s in enumerate(sents, 1):
        text = s["text"]
        if not text.strip():
            continue

        cand = llm_candidate_entities(text)
        # æ¸…æ´— + åˆ—è¡¨æ‹†åˆ† + è§„åˆ™è¿‡æ»¤ + span è¿‡æ»¤
        cand2 = []
        for c in cand:
            m0 = clean_mention(c["mention"])
            for m in split_list(m0):
                if is_bad(m):
                    continue
                if find_span(text, m) is None:
                    continue
                cand2.append({"mention": m, "ent_type": c["ent_type"]})

        # å»é‡ä¿åº
        seen = set()
        cand3 = []
        for c in cand2:
            if c["mention"] not in seen:
                seen.add(c["mention"])
                cand3.append(c)

        keep = llm_verify_entities(text, cand3)

        for k in keep:
            m = clean_mention(k["mention"])
            sp = find_span(text, m)
            if sp is None:
                continue
            key = (s["page_no"], m, k["ent_type"])
            if key in seen_global:
                continue
            seen_global.add(key)

            out.append({
                "entity_id": f"e{eid:05d}",
                "sentence_id": s["sentence_id"],
                "page_no": s["page_no"],
                "mention": m,
                "start_char": sp[0],
                "end_char": sp[1],
                "ent_type": k["ent_type"],
                "confidence": 0.95,
            })
            eid += 1

        if i % 50 == 0:
            print(f"â€¦processed {i}/{len(sents)}  truth_entities={len(out)}")

    save_tsv(out, truth_entity_tsv_path)


if __name__ == "__main__":
    main()
