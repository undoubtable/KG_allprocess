import os
import csv
import json
import yaml
import time
from typing import List, Dict, Any, Optional, Tuple
from collections import defaultdict

from openai import OpenAI

from pipeline_config import STEP2_SENT_TSV, STEP35_TRUTH_ENT_TSV, STEP45_NODES_TSV, STEP45_EDGES_TSV

# ===================== è·¯å¾„é…ç½® =====================
sent_tsv_path = str(STEP2_SENT_TSV)

# Truth-å®ä½“è¾“å‡ºï¼ˆæ¥è‡ª Truth_Entity_verbose / Truth_Entity_fastï¼‰
truth_entity_tsv_path = str(STEP35_TRUTH_ENT_TSV)

# Truth-å…³ç³»è¾“å‡ºï¼ˆnodes / edgesï¼‰
nodes_truth_path = str(STEP45_NODES_TSV)
edges_truth_path = str(STEP45_EDGES_TSV)
# ===================== LLM é…ç½®ï¼ˆæ²¿ç”¨ä½ çš„ Gitee AI OpenAIå…¼å®¹è°ƒç”¨ï¼‰ =====================
with open("config.yaml", "r", encoding="utf-8") as f:
    config = yaml.safe_load(f)

client = OpenAI(
    base_url="https://ai.gitee.com/v1",
    api_key=config["api_key"],
    default_headers={"X-Failover-Enabled": "true"},
)

CANDIDATE_MODEL = "DeepSeek-V3"
VERIFY_MODEL = "DeepSeek-R1"

# ===================== å…³ç³»ç±»å‹é›†åˆï¼ˆtruth æ¨èå°è€Œç¨³ï¼‰ =====================
REL_TYPES = [
    "defines",      # å®šä¹‰/è§£é‡Š/å±äºâ€¦çš„å«ä¹‰
    "includes",     # åŒ…å«/ç»„æˆ/åŒ…æ‹¬
    "part_of",      # å±äº/éš¶å±/æ„æˆâ€¦çš„ä¸€éƒ¨åˆ†
    "causes",       # å¯¼è‡´/å¼•èµ·
    "applies_to",   # é€‚ç”¨/é’ˆå¯¹
    "punishes",     # å¤„ç½š/å®šç½ªï¼ˆåˆ‘æ³•/ç½ªååœºæ™¯ï¼‰
    "related_to",   # å…œåº•ï¼ˆtruth ä¸€èˆ¬ä¼šå°½é‡ä¸ä¿ç•™ï¼‰
]

# ===================== FAST æ ¸å¿ƒå‚æ•° =====================
BATCH_SIZE = 8  # âœ… æ¯æ‰¹å¥å­æ•°ï¼ˆæ¨è 6~12ï¼Œæ ¹æ®æ¥å£ç¨³å®šæ€§è°ƒæ•´ï¼‰
PRINT_FIRST_N_BATCH = 1
PRINT_EVERY_N_BATCH = 5

# å¥å­å†…æ§é‡ï¼ˆå¾ˆå…³é”®ï¼Œé¿å… prompt çˆ†é•¿/å˜æ…¢ï¼‰
MAX_MENTIONS_PER_SENT = 10       # æ¯å¥æœ€å¤šç”¨å¤šå°‘ä¸ªå®ä½“å‚ä¸å…³ç³»æŠ½å–
MAX_REL_CAND_PER_SENT = 10       # æ¯å¥ V3 æœ€å¤šå€™é€‰å…³ç³»
MAX_REL_KEEP_PER_SENT = 6        # æ¯å¥ R1 æœ€å¤šä¿ç•™å…³ç³»

EDGE_CONF_TRUTH = 0.95

# é™åˆ¶â€œå€™é€‰å…³ç³»æ€»é‡â€ï¼Œé¿å… batch payload è¿‡å¤§
MAX_TOTAL_CAND_REL_PER_BATCH = 120

# é‡è¯•é…ç½®
MAX_RETRIES = 3
RETRY_BASE_SLEEP = 1.2

# ===================== I/O è¯»å– =====================
def load_sentences(tsv_path: str) -> Dict[str, Dict[str, Any]]:
    sents = {}
    with open(tsv_path, "r", encoding="utf-8") as f:
        _ = f.readline()
        for line in f:
            parts = line.rstrip("\n").split("\t")
            if len(parts) < 3:
                continue
            sid, page_no, text = parts[0], int(parts[1]), parts[2]
            sents[sid] = {"sentence_id": sid, "page_no": page_no, "text": text}
    return sents


def load_entities(tsv_path: str) -> List[Dict[str, Any]]:
    ents = []
    with open(tsv_path, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f, delimiter="\t")
        for row in reader:
            row["page_no"] = int(row.get("page_no", 0))
            row["start_char"] = int(row.get("start_char", 0))
            row["end_char"] = int(row.get("end_char", 0))
            try:
                row["confidence"] = float(row.get("confidence", 0.0) or 0.0)
            except Exception:
                row["confidence"] = 0.0
            ents.append(row)
    return ents


# ===================== JSON æå– =====================
def extract_json(content: str) -> Optional[Dict[str, Any]]:
    content = (content or "").strip()
    l = content.find("{")
    r = content.rfind("}")
    if l == -1 or r == -1 or r <= l:
        return None
    try:
        return json.loads(content[l:r + 1])
    except Exception:
        return None


# ===================== å¯é è¯·æ±‚å°è£…ï¼ˆå¸¦é‡è¯•ï¼‰ =====================
def chat_with_retry(model: str, messages: List[Dict[str, str]], temperature: float = 0.1) -> str:
    last_err = None
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            resp = client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
            )
            return (resp.choices[0].message.content or "").strip()
        except Exception as e:
            last_err = e
            sleep_s = RETRY_BASE_SLEEP * attempt
            print(f"âš ï¸ LLM è¯·æ±‚å¤±è´¥ï¼ˆ{model}ï¼‰attempt {attempt}/{MAX_RETRIES}: {e} -> sleep {sleep_s:.1f}s")
            time.sleep(sleep_s)
    raise RuntimeError(f"LLM è¯·æ±‚å¤±è´¥ï¼ˆé‡è¯•ä»å¤±è´¥ï¼‰ï¼š{last_err}")


# ===================== Truth èŠ‚ç‚¹åˆå¹¶ï¼ˆä¸ Step4 ä¸€è‡´ï¼‰ =====================
def build_unique_nodes(entities: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], Dict[Tuple[str, str], str]]:
    key2node_id = {}
    nodes = []
    idx = 1

    for e in entities:
        key = (e["mention"], e.get("ent_type", "Other"))
        if key not in key2node_id:
            node_id = f"n{idx:05d}"
            key2node_id[key] = node_id
            idx += 1
            nodes.append({
                "node_id": node_id,
                "name": e["mention"],
                "label": e.get("ent_type", "Other"),
                "page_no": e.get("page_no", ""),
                "sentence_id": e.get("sentence_id", ""),
            })

    return nodes, key2node_id


# ===================== Batchï¼šV3 å€™é€‰å…³ç³» =====================
def candidate_relations_batch(items: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, str]]]:
    """
    items: [{"sentence_id","text","mentions"}...]
    return: {sentence_id: [{"head","rel","tail"}, ...], ...}
    """
    system = (
        "ä½ æ˜¯ä¸­æ–‡å…³ç³»æŠ½å–åŠ©æ‰‹ã€‚è¯·å¯¹å¤šä¸ªå¥å­åˆ†åˆ«æŠ½å–å®ä½“å…³ç³»ã€‚\n"
        "è¾“å‡ºå¿…é¡»æ˜¯ä¸¥æ ¼ JSONï¼ˆä¸è¦ Markdownï¼Œä¸è¦è§£é‡Šï¼‰ï¼Œæ ¼å¼ï¼š\n"
        "{\"items\":[{\"sentence_id\":\"...\",\"relations\":[{\"head\":\"...\",\"rel\":\"...\",\"tail\":\"...\"}, ...]}, ...]}\n"
        "ç¡¬è§„åˆ™ï¼š\n"
        "1) head/tail å¿…é¡»ä¸¥æ ¼ä»è¯¥å¥çš„å®ä½“åˆ—è¡¨ä¸­é€‰æ‹©ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰ã€‚\n"
        f"2) rel åªèƒ½ä» {REL_TYPES} ä¸­é€‰æ‹©ã€‚\n"
        "3) åªæŠ½å–å¥å­é‡Œæœ‰æ˜ç¡®è¯­è¨€è¯æ®æ”¯æŒçš„å…³ç³»ï¼›ä¸ç¡®å®šä¸è¦è¾“å‡ºã€‚\n"
        f"4) æ¯å¥æœ€å¤šè¾“å‡º {MAX_REL_CAND_PER_SENT} æ¡å…³ç³»ã€‚\n"
    )
    payload = {
        "items": [
            {"sentence_id": x["sentence_id"], "text": x["text"], "mentions": x["mentions"]}
            for x in items
        ]
    }

    content = chat_with_retry(
        model=CANDIDATE_MODEL,
        messages=[
            {"role": "system", "content": system},
            {"role": "user", "content": json.dumps(payload, ensure_ascii=False)},
        ],
        temperature=0.1,
    )

    data = extract_json(content) or {}
    out: Dict[str, List[Dict[str, str]]] = {}
    items_out = data.get("items", [])
    if not isinstance(items_out, list):
        return out

    for it in items_out:
        if not isinstance(it, dict):
            continue
        sid = str(it.get("sentence_id", "")).strip()
        rels = it.get("relations", [])
        if not sid or not isinstance(rels, list):
            continue
        cleaned = []
        for r in rels:
            if not isinstance(r, dict):
                continue
            h = str(r.get("head", "")).strip()
            rel = str(r.get("rel", "")).strip()
            t = str(r.get("tail", "")).strip()
            if h and t and rel:
                cleaned.append({"head": h, "rel": rel, "tail": t})
        out[sid] = cleaned[:MAX_REL_CAND_PER_SENT]
    return out


# ===================== Batchï¼šR1 æ ¡éªŒå…³ç³» =====================
def verify_relations_batch(items: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, str]]]:
    """
    items: [{"sentence_id","text","mentions","candidates":[{head,rel,tail}...]}...]
    return: {sentence_id: [{"head","rel","tail"}, ...], ...}
    """
    system = (
        "ä½ æ˜¯å…³ç³»æ ¡éªŒå™¨ã€‚è¯·å¯¹å¤šä¸ªå¥å­åˆ†åˆ«ä»å€™é€‰å…³ç³»ä¸­ç­›é€‰åº”ä¿ç•™çš„å…³ç³»ã€‚\n"
        "è¾“å‡ºå¿…é¡»æ˜¯ä¸¥æ ¼ JSONï¼ˆä¸è¦ Markdownï¼Œä¸è¦è§£é‡Šï¼‰ï¼Œæ ¼å¼ï¼š\n"
        "{\"items\":[{\"sentence_id\":\"...\",\"relations\":[{\"head\":\"...\",\"rel\":\"...\",\"tail\":\"...\"}, ...]}, ...]}\n"
        "ç¡¬è§„åˆ™ï¼š\n"
        "1) åªèƒ½ä»è¯¥å¥çš„å€™é€‰å…³ç³»ä¸­é€‰æ‹©ï¼ˆhead/rel/tail å¿…é¡»å®Œå…¨ä¸€è‡´ï¼‰ã€‚\n"
        f"2) rel åªèƒ½ä» {REL_TYPES} ä¸­é€‰æ‹©ã€‚\n"
        "3) åªä¿ç•™å¥å­é‡Œæœ‰æ˜ç¡®è¯æ®æ”¯æŒçš„å…³ç³»ï¼›è¯æ®ä¸è¶³ä¸è¦ä¿ç•™ã€‚\n"
        f"4) æ¯å¥æœ€å¤šä¿ç•™ {MAX_REL_KEEP_PER_SENT} æ¡å…³ç³»ã€‚\n"
        "å®å¯å°‘ç•™ï¼Œä¸è¦é”™ç•™ã€‚\n"
    )
    payload = {"items": items}

    content = chat_with_retry(
        model=VERIFY_MODEL,
        messages=[
            {"role": "system", "content": system},
            {"role": "user", "content": json.dumps(payload, ensure_ascii=False)},
        ],
        temperature=0.1,
    )

    data = extract_json(content) or {}
    out: Dict[str, List[Dict[str, str]]] = {}
    items_out = data.get("items", [])
    if not isinstance(items_out, list):
        return out

    for it in items_out:
        if not isinstance(it, dict):
            continue
        sid = str(it.get("sentence_id", "")).strip()
        rels = it.get("relations", [])
        if not sid or not isinstance(rels, list):
            continue
        cleaned = []
        for r in rels:
            if not isinstance(r, dict):
                continue
            h = str(r.get("head", "")).strip()
            rel = str(r.get("rel", "")).strip()
            t = str(r.get("tail", "")).strip()
            if h and t and rel:
                cleaned.append({"head": h, "rel": rel, "tail": t})
        out[sid] = cleaned[:MAX_REL_KEEP_PER_SENT]
    return out


# ===================== ä¿å­˜ TSV =====================
def save_tsv(rows: List[Dict[str, Any]], fields: List[str], path: str) -> None:
    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
    with open(path, "w", encoding="utf-8", newline="") as f:
        w = csv.DictWriter(f, fieldnames=fields, delimiter="\t")
        w.writeheader()
        for r in rows:
            w.writerow(r)
    print(f"âœ… saved: {path} ({len(rows)} rows)")


# ===================== ä¸»æµç¨‹ =====================
def main():
    print("========== Truth-Relation FASTï¼ˆbatchï¼šV3 å€™é€‰ + R1 æ ¡éªŒï¼‰==========")
    print(f"ğŸ¤– å€™é€‰æ¨¡å‹ï¼š{CANDIDATE_MODEL} | æ ¡éªŒæ¨¡å‹ï¼š{VERIFY_MODEL}")
    print(f"âš™ï¸ BATCH_SIZE={BATCH_SIZE}")
    print(f"âš™ï¸ MAX_MENTIONS_PER_SENT={MAX_MENTIONS_PER_SENT}, MAX_REL_CAND_PER_SENT={MAX_REL_CAND_PER_SENT}, MAX_REL_KEEP_PER_SENT={MAX_REL_KEEP_PER_SENT}")
    print(f"ğŸ“Œ REL_TYPES={REL_TYPES}\n")

    sentences = load_sentences(sent_tsv_path)
    print(f"ğŸ“„ å·²åŠ è½½å¥å­æ•°ï¼š{len(sentences)}")

    entities = load_entities(truth_entity_tsv_path)
    print(f"ğŸ“„ å·²åŠ è½½ truth å®ä½“æ•°ï¼š{len(entities)}")

    # nodesï¼ˆæŒ‰ mention+type åˆå¹¶ï¼‰
    nodes, key2node_id = build_unique_nodes(entities)
    print(f"âœ¨ åˆå¹¶å truth èŠ‚ç‚¹æ•°ï¼š{len(nodes)}")

    # sentence -> entitiesï¼ˆæŒ‰ start_char æ’åºï¼‰
    ents_by_sent = defaultdict(list)
    for e in entities:
        ents_by_sent[e["sentence_id"]].append(e)

    # ç»Ÿä¸€æˆå¯è¿­ä»£åˆ—è¡¨ï¼ˆåªå¤„ç†æœ‰ >=2 ä¸ªå®ä½“çš„å¥å­ï¼‰
    sent_items = []
    for sid, ents in ents_by_sent.items():
        sent = sentences.get(sid)
        if not sent:
            continue
        if len(ents) < 2:
            continue
        ents_sorted = sorted(ents, key=lambda x: x["start_char"])

        # mentions å»é‡ä¿åº + æˆªæ–­
        mentions = []
        mention2type = {}
        for e in ents_sorted:
            m = e["mention"]
            if m not in mentions:
                mentions.append(m)
            mention2type.setdefault(m, e.get("ent_type", "Other"))
        if len(mentions) > MAX_MENTIONS_PER_SENT:
            mentions = mentions[:MAX_MENTIONS_PER_SENT]

        sent_items.append({
            "sentence_id": sid,
            "page_no": sent["page_no"],
            "text": sent["text"],
            "mentions": mentions,
            "mention2type": mention2type,  # ä¹‹åæ˜ å°„ node_id ç”¨
        })

    print(f"ğŸ§© æ»¡è¶³å…³ç³»æŠ½å–æ¡ä»¶çš„å¥å­æ•°ï¼ˆ>=2 å®ä½“ï¼‰ï¼š{len(sent_items)}")
    total_batches = (len(sent_items) + BATCH_SIZE - 1) // BATCH_SIZE
    print(f"ğŸš€ å°†åˆ† {total_batches} ä¸ª batch å¤„ç†ï¼ˆæ¯ batch {BATCH_SIZE} å¥ï¼‰\n")

    edges: List[Dict[str, Any]] = []
    seen_edge = set()
    edge_idx = 1

    # batch å¾ªç¯
    for bi in range(total_batches):
        batch = sent_items[bi * BATCH_SIZE: (bi + 1) * BATCH_SIZE]

        # -------- 1) V3 å€™é€‰å…³ç³»ï¼ˆbatch ä¸€æ¬¡ï¼‰--------
        cand_input = [{"sentence_id": x["sentence_id"], "text": x["text"], "mentions": x["mentions"]} for x in batch]
        cand_map = candidate_relations_batch(cand_input)

        # -------- 2) æ¸…æ´—å€™é€‰å…³ç³» + æ§åˆ¶ batch æ€»é‡ --------
        verify_items = []
        total_cand_rel = 0

        for x in batch:
            sid = x["sentence_id"]
            mentions = x["mentions"]
            mention_set = set(mentions)

            cand = cand_map.get(sid, []) or []
            # å¼ºçº¦æŸï¼šhead/tail å¿…é¡»åœ¨ mentions å†…ï¼Œrel åˆæ³•ï¼Œä¸” head!=tail
            cand2 = []
            for r in cand:
                h, rel, t = r["head"], r["rel"], r["tail"]
                if h in mention_set and t in mention_set and rel in REL_TYPES and h != t:
                    cand2.append({"head": h, "rel": rel, "tail": t})

            # å»é‡ä¿åº
            seen_local = set()
            dedup = []
            for r in cand2:
                k = (r["head"], r["rel"], r["tail"])
                if k not in seen_local:
                    seen_local.add(k)
                    dedup.append(r)

            # æ¯å¥æˆªæ–­
            dedup = dedup[:MAX_REL_CAND_PER_SENT]

            # batch æ€»é‡æ§åˆ¶ï¼šå¤ªå¤šå°±å†ç 
            if total_cand_rel + len(dedup) > MAX_TOTAL_CAND_REL_PER_BATCH:
                remain = max(0, MAX_TOTAL_CAND_REL_PER_BATCH - total_cand_rel)
                dedup = dedup[:remain]

            total_cand_rel += len(dedup)

            verify_items.append({
                "sentence_id": sid,
                "text": x["text"],
                "mentions": mentions,
                # æ³¨æ„ï¼šæ ¡éªŒæ—¶è®©æ¨¡å‹åªä»å€™é€‰é‡Œé€‰
                "candidates": dedup,
            })

        # -------- 3) R1 æ ¡éªŒå…³ç³»ï¼ˆbatch ä¸€æ¬¡ï¼‰--------
        keep_map = verify_relations_batch(verify_items)

        # -------- 4) ç”Ÿæˆ edges --------
        # æ‰“å°æ ·ä¾‹ï¼ˆå‰å‡ ä¸ª batchï¼‰
        if bi < PRINT_FIRST_N_BATCH:
            print(f"=== batch {bi+1}/{total_batches} ç¤ºä¾‹ ===")
            for x in batch[:3]:
                sid = x["sentence_id"]
                cand_show = cand_map.get(sid, [])
                keep_show = keep_map.get(sid, [])
                print(f"[sid={sid}]")
                print("TEXT:", x["text"])
                print("MENTIONS:", x["mentions"])
                print("CAND_REL:", [f"{r['head']}--{r['rel']}-->{r['tail']}" for r in cand_show[:8]])
                print("KEEP_REL:", [f"{r['head']}--{r['rel']}-->{r['tail']}" for r in keep_show[:8]])
                print()

        for x in batch:
            sid = x["sentence_id"]
            page_no = x["page_no"]
            mention2type = x["mention2type"]
            mentions = x["mentions"]

            # mention -> node_id
            mention2nid = {}
            for m in mentions:
                t = mention2type.get(m, "Other")
                nid = key2node_id.get((m, t))
                if nid:
                    mention2nid[m] = nid

            # å€™é€‰ setï¼ˆç”¨äºå¼ºçº¦æŸï¼škeep å¿…é¡»æ¥è‡ª candidatesï¼‰
            cand_list = (verify_items[[i for i, it in enumerate(verify_items) if it["sentence_id"] == sid][0]]["candidates"]
                         if any(it["sentence_id"] == sid for it in verify_items) else [])
            cand_set = {(r["head"], r["rel"], r["tail"]) for r in cand_list}

            keep = keep_map.get(sid, []) or []
            # å¼ºçº¦æŸï¼šåªä¿ç•™æ¥è‡ªå€™é€‰çš„
            keep2 = []
            for r in keep:
                k = (r.get("head"), r.get("rel"), r.get("tail"))
                if k in cand_set and r.get("rel") in REL_TYPES:
                    keep2.append({"head": r["head"], "rel": r["rel"], "tail": r["tail"]})

            # å»é‡ä¿åº + æ¯å¥æˆªæ–­
            seen_local = set()
            final_keep = []
            for r in keep2:
                k = (r["head"], r["rel"], r["tail"])
                if k not in seen_local:
                    seen_local.add(k)
                    final_keep.append(r)
            final_keep = final_keep[:MAX_REL_KEEP_PER_SENT]

            for r in final_keep:
                src = mention2nid.get(r["head"])
                dst = mention2nid.get(r["tail"])
                if not src or not dst or src == dst:
                    continue
                k = (src, r["rel"], dst, sid)
                if k in seen_edge:
                    continue
                seen_edge.add(k)

                edges.append({
                    "edge_id": f"e{edge_idx:05d}",
                    "src_id": src,
                    "dst_id": dst,
                    "relation_type": r["rel"],
                    "page_no": page_no,
                    "sentence_id": sid,
                    "confidence": EDGE_CONF_TRUTH,
                })
                edge_idx += 1

        if (bi + 1) % PRINT_EVERY_N_BATCH == 0:
            print(f"â€¦å·²å¤„ç† batch {bi+1}/{total_batches} | truth_edges={len(edges)}")

    print("\nğŸ“Œ truth è¾¹ç¤ºä¾‹ï¼ˆå‰10æ¡ï¼‰ï¼š")
    for e in edges[:10]:
        print(e)

    # ä¿å­˜
    save_tsv(nodes, ["node_id", "name", "label", "page_no", "sentence_id"], nodes_truth_path)
    save_tsv(edges, ["edge_id", "src_id", "dst_id", "relation_type", "page_no", "sentence_id", "confidence"], edges_truth_path)

    print("\n========== Done ==========")


if __name__ == "__main__":
    main()
