import os
import csv
import json
import yaml
from collections import defaultdict
from typing import Dict, Any, List, Optional, Tuple

from openai import OpenAI

from pipeline_config import STEP2_SENT_TSV, STEP3_ENT_TSV, STEP4_NODES_TSV, STEP4_EDGES_TSV

sent_tsv_path = str(STEP2_SENT_TSV)
entity_tsv_path = str(STEP3_ENT_TSV)
out_nodes_path = str(STEP4_NODES_TSV)
out_edges_path = str(STEP4_EDGES_TSV)

# ======== LLM é…ç½®ï¼ˆæ²¿ç”¨ä½ çš„ Gitee AI OpenAIå…¼å®¹è°ƒç”¨ï¼‰========
with open("config.yaml", "r", encoding="utf-8") as f:
    config = yaml.safe_load(f)

client = OpenAI(
    base_url="https://ai.gitee.com/v1",
    api_key=config["api_key"],
    default_headers={"X-Failover-Enabled": "true"},
)

MODEL_NAME = "DeepSeek-V3"

# ======== å…³ç³»ç±»å‹é›†åˆï¼ˆKG-version1 å…ˆç”¨å°é›†åˆï¼Œä¾¿äºæ§å™ªï¼‰========
REL_TYPES = [
    "defines",      # å®šä¹‰/è§£é‡Š
    "includes",     # åŒ…å«/ç»„æˆ
    "part_of",      # å±äº/éš¶å±
    "causes",       # å¯¼è‡´/å¼•èµ·
    "applies_to",   # é€‚ç”¨/é’ˆå¯¹
    "related_to"    # å…œåº•ï¼ˆä¸ç¡®å®šæ—¶ï¼‰
]

# æ§åˆ¶ LLM è°ƒç”¨é‡ï¼šæ¯å¥åªå¯¹ç›¸é‚»å®ä½“å¯¹åšä¸€æ¬¡å…³ç³»åˆ¤åˆ«
MAX_EDGES_PER_SENT = 20  # å¥å­å¤ªé•¿æ—¶ï¼Œæœ€å¤šå¤„ç†å‰ N å¯¹ç›¸é‚»å®ä½“

DEFAULT_EDGE_CONF = 0.80


def load_sentences(tsv_path: str) -> Dict[str, Dict[str, Any]]:
    """
    è¯»å– Step2 å¥å­ TSVï¼šsentence_id | page_no | text
    """
    sents = {}
    with open(tsv_path, "r", encoding="utf-8") as f:
        _ = f.readline()
        for line in f:
            parts = line.rstrip("\n").split("\t")
            if len(parts) < 3:
                continue
            sid, page_no, text = parts[0], int(parts[1]), parts[2]
            sents[sid] = {"sentence_id": sid, "page_no": page_no, "text": text}
    return sents


def load_entities(tsv_path: str) -> List[Dict[str, Any]]:
    """
    è¯»å– Step3 çš„å®ä½“ TSV
    entity_id, sentence_id, page_no, mention, start_char, end_char, ent_type, confidence
    """
    entities = []
    with open(tsv_path, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f, delimiter="\t")
        for row in reader:
            row["page_no"] = int(row.get("page_no", 0))
            row["start_char"] = int(row.get("start_char", 0))
            row["end_char"] = int(row.get("end_char", 0))
            try:
                row["confidence"] = float(row.get("confidence", 0.0))
            except ValueError:
                row["confidence"] = 0.0
            entities.append(row)
    return entities


def build_unique_nodes(entities: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], Dict[Tuple[str, str], str]]:
    """
    æŒ‰ (mention, ent_type) åˆå¹¶å®ä½“ï¼Œç”Ÿæˆå”¯ä¸€èŠ‚ç‚¹ï¼ˆä¸ä½ åŸ Step4 ä¸€è‡´ï¼‰:contentReference[oaicite:2]{index=2}
    """
    key2node_id = {}
    nodes = []
    idx = 1

    for e in entities:
        mention = e["mention"]
        ent_type = e.get("ent_type", "Entity")
        key = (mention, ent_type)

        if key not in key2node_id:
            node_id = f"n{idx:05d}"
            key2node_id[key] = node_id
            idx += 1

            nodes.append(
                {
                    "node_id": node_id,
                    "name": mention,
                    "label": ent_type,
                    "page_no": e.get("page_no", ""),
                    "sentence_id": e.get("sentence_id", ""),
                }
            )

    return nodes, key2node_id


def _extract_json(content: str) -> Optional[Dict[str, Any]]:
    content = (content or "").strip()
    l = content.find("{")
    r = content.rfind("}")
    if l == -1 or r == -1 or r <= l:
        return None
    try:
        return json.loads(content[l:r + 1])
    except Exception:
        return None


def llm_classify_relation(sentence: str, head: Dict[str, Any], tail: Dict[str, Any]) -> Tuple[str, float]:
    """
    ç»™å®šå¥å­ä¸ä¸¤ä¸ªå®ä½“ï¼ˆç›¸é‚»ï¼‰ï¼Œè®© LLM é€‰æ‹© relation_type
    è¾“å‡ºï¼šrelation_type, confidence
    """
    h_m = head["mention"]
    t_m = tail["mention"]
    h_t = head.get("ent_type", "Other")
    t_t = tail.get("ent_type", "Other")

    system_prompt = (
        "ä½ æ˜¯ä¸­æ–‡å…³ç³»æŠ½å–åŠ©æ‰‹ã€‚ç»™å®šå¥å­å’Œä¸¤ä¸ªå®ä½“ï¼Œè¯·åˆ¤æ–­äºŒè€…åœ¨å¥å­ä¸­æ˜¯å¦å­˜åœ¨æ˜ç¡®å…³ç³»ã€‚\n"
        "è¾“å‡ºå¿…é¡»æ˜¯ä¸¥æ ¼ JSONï¼ˆä¸è¦ Markdownï¼Œä¸è¦è§£é‡Šï¼‰ï¼Œæ ¼å¼ï¼š\n"
        "{\"relation_type\":\"...\",\"confidence\":0.0}\n"
        "è§„åˆ™ï¼š\n"
        f"1) relation_type åªèƒ½ä» {REL_TYPES} ä¸­é€‰æ‹©ã€‚\n"
        "2) å¦‚æœå…³ç³»ä¸æ˜ç¡®ï¼Œè¯·è¾“å‡º related_toï¼Œå¹¶ç»™è¾ƒä½ confidenceã€‚\n"
        "3) confidence å– 0~1ï¼Œè¶Šç¡®å®šè¶Šé«˜ã€‚\n"
        "4) ä¸è¦ç¼–é€ å¥å­å¤–çŸ¥è¯†ï¼Œåªæ ¹æ®å¥å­ã€‚\n"
    )

    user_prompt = (
        f"å¥å­ï¼š{sentence}\n"
        f"å®ä½“Aï¼š{h_m}ï¼ˆ{h_t}ï¼‰\n"
        f"å®ä½“Bï¼š{t_m}ï¼ˆ{t_t}ï¼‰\n"
        "è¯·è¾“å‡º JSONï¼š"
    )

    resp = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[{"role": "system", "content": system_prompt},
                  {"role": "user", "content": user_prompt}],
        temperature=0.1,
    )

    data = _extract_json(resp.choices[0].message.content)
    if not data:
        return "related_to", DEFAULT_EDGE_CONF

    rel = str(data.get("relation_type", "related_to")).strip()
    if rel not in REL_TYPES:
        rel = "related_to"

    try:
        conf = float(data.get("confidence", DEFAULT_EDGE_CONF))
    except Exception:
        conf = DEFAULT_EDGE_CONF

    # clamp
    conf = max(0.0, min(1.0, conf))
    return rel, conf


def build_edges_by_sentence_llm(
    sentences: Dict[str, Dict[str, Any]],
    entities: List[Dict[str, Any]],
    key2node_id: Dict[Tuple[str, str], str],
) -> List[Dict[str, Any]]:
    """
    KG-version1ï¼šåŒå¥å†…æŒ‰ start_char æ’åºï¼Œåªå¯¹ç›¸é‚»å®ä½“å¯¹è¿è¾¹ï¼ˆç»“æ„ä¸åŸ Step4 ä¸€è‡´ï¼‰:contentReference[oaicite:3]{index=3}
    ä½† relation_type ç”± LLM åˆ¤åˆ«ã€‚
    """
    ents_by_sent = defaultdict(list)
    for e in entities:
        ents_by_sent[e["sentence_id"]].append(e)

    edges = []
    edge_idx = 1

    for si, (sent_id, ents) in enumerate(ents_by_sent.items(), start=1):
        sent = sentences.get(sent_id)
        if not sent:
            continue

        ents_sorted = sorted(ents, key=lambda x: x["start_char"])
        if len(ents_sorted) < 2:
            continue

        page_no = sent["page_no"]
        text = sent["text"]

        pair_count = 0
        for i in range(len(ents_sorted) - 1):
            if pair_count >= MAX_EDGES_PER_SENT:
                break

            h = ents_sorted[i]
            t = ents_sorted[i + 1]

            h_key = (h["mention"], h.get("ent_type", "Entity"))
            t_key = (t["mention"], t.get("ent_type", "Entity"))

            src = key2node_id.get(h_key)
            dst = key2node_id.get(t_key)
            if not src or not dst or src == dst:
                continue

            rel_type, rel_conf = llm_classify_relation(text, h, t)

            edges.append(
                {
                    "edge_id": f"e{edge_idx:05d}",
                    "src_id": src,
                    "dst_id": dst,
                    "relation_type": rel_type,
                    "page_no": page_no,
                    "sentence_id": sent_id,
                    # è¾¹ç½®ä¿¡åº¦ = min(å®ä½“ç½®ä¿¡åº¦) ä¸ å…³ç³»ç½®ä¿¡åº¦ çš„ç»„åˆï¼ˆä¿å®ˆï¼‰
                    "confidence": min(h["confidence"], t["confidence"], rel_conf),
                }
            )
            edge_idx += 1
            pair_count += 1

        if si % 50 == 0:
            print(f"â€¦å·²å¤„ç†å¥å­ {si}/{len(ents_by_sent)}ï¼Œå½“å‰è¾¹æ•°ï¼š{len(edges)}")

    return edges


def save_tsv(rows: List[Dict[str, Any]], fieldnames: List[str], path: str) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter="\t")
        writer.writeheader()
        for r in rows:
            writer.writerow(r)
    print(f"âœ… å·²ä¿å­˜ï¼š{path}ï¼ˆ{len(rows)} è¡Œï¼‰")


def main():
    print("========== Step4 KG-version1ï¼ˆLLM å…³ç³»æŠ½å–ï¼‰==========")

    # 1) è¯»å–å¥å­ï¼ˆç”¨äº LLM åšå…³ç³»åˆ¤åˆ«ï¼‰
    sentences = load_sentences(sent_tsv_path)
    print(f"ğŸ“„ å¥å­æ€»æ•°ï¼š{len(sentences)}")

    # 2) è¯»å–å®ä½“
    entities = load_entities(entity_tsv_path)
    print(f"ğŸ“„ å®ä½“æ€»æ•°ï¼ˆåŒ…æ‹¬é‡å¤æåŠï¼‰ï¼š{len(entities)}")

    # 3) åˆå¹¶æˆå”¯ä¸€èŠ‚ç‚¹
    nodes, key2node_id = build_unique_nodes(entities)
    print(f"âœ¨ åˆå¹¶åå”¯ä¸€å®ä½“ï¼ˆèŠ‚ç‚¹ï¼‰æ•°é‡ï¼š{len(nodes)}")

    # 4) å¥å­å†…ç›¸é‚»å®ä½“ â†’ LLM åˆ¤åˆ«å…³ç³»ç±»å‹ â†’ ç”Ÿæˆè¾¹
    print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹ï¼š{MODEL_NAME} è¿›è¡Œ relation_type åˆ¤åˆ«")
    edges = build_edges_by_sentence_llm(sentences, entities, key2node_id)
    print(f"ğŸ”— ç”Ÿæˆè¾¹æ•°é‡ï¼š{len(edges)}")

    # 5) æ‰“å°è¾¹æ ·ä¾‹
    print("\nğŸ“Œ è¾¹ç¤ºä¾‹ï¼ˆå‰10æ¡ï¼‰ï¼š")
    for e in edges[:10]:
        print(e)

    # 6) ä¿å­˜
    save_tsv(nodes, ["node_id", "name", "label", "page_no", "sentence_id"], out_nodes_path)
    save_tsv(edges, ["edge_id", "src_id", "dst_id", "relation_type", "page_no", "sentence_id", "confidence"], out_edges_path)

    print("========== Done ==========")


if __name__ == "__main__":
    main()
