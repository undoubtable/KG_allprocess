import os
import re
import csv
import json
import time
import yaml
from collections import defaultdict
from typing import List, Dict, Any, Optional, Tuple

from openai import OpenAI

# å¦‚æœä½ æœ¬åœ°æ²¡æœ‰pipeline_config.pyï¼Œæ³¨é‡Šæ‰ä¸‹é¢ä¸¤è¡Œï¼Œæ”¹ç”¨æ‰‹åŠ¨é…ç½®è·¯å¾„
from pipeline_config import STEP2_SENT_TSV, STEP3_ENT_TSV

# ========= è·¯å¾„é…ç½® =========
sent_tsv_path = str(STEP2_SENT_TSV)
output_entity_path = str(STEP3_ENT_TSV)

# sent_tsv_path = r"D:\...\Step2_output\å¥å­åˆ—è¡¨.tsv"
# output_entity_path = r"D:\...\Step3_output\å®ä½“åˆ—è¡¨.tsv"
import yaml
# ========= LLM é…ç½®ï¼ˆæ²¿ç”¨ä½ çš„ Gitee AI è°ƒç”¨æ–¹å¼ï¼‰ =========
with open("config.yaml", "r", encoding="utf-8") as f:
    config = yaml.safe_load(f)

client = OpenAI(
    base_url="https://ai.gitee.com/v1",
    api_key=config["api_key"],
    default_headers={"X-Failover-Enabled": "true"},
)

MODEL_NAME = "DeepSeek-V3"  # å®ä½“æŠ½å–æ¨èç”¨ V3

# ========= æŠ½å–ä¸è¿‡æ»¤å‚æ•° =========
min_char_len = 2

# æ¯å¥æœ€å¤šä¿ç•™å¤šå°‘å®ä½“ï¼ˆæ§é‡æ ¸å¿ƒï¼‰
MAX_ENT_PER_SENT = 8

# LLM æ²¡æœ‰ç¨³å®š token-level scoreï¼šè¿™é‡Œç”¨é»˜è®¤å€¼ï¼›ä½ ä¹Ÿå¯ä»¥åç»­è®©æ¨¡å‹è¾“å‡º salience å†™å…¥ confidence
DEFAULT_CONF = 0.85

# ä¸­æ–‡æ•°å­— + é˜¿æ‹‰ä¼¯æ•°å­—ï¼Œç”¨äºè¿‡æ»¤â€œçº¯æ•°å­—å®ä½“â€
CH_NUMERIC_CHARS = set("ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶ã€‡0ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™0123456789")

# æ³›åŒ–/å™ªå£°è¯ï¼ˆå¯æŒç»­æ‰©å……ï¼‰
BAD_GENERIC = {
    "è¡Œä¸º","è§„å®š","æƒ…å†µ","æ–¹é¢","é—®é¢˜","è¿‡ç¨‹","å†…å®¹","æ–¹å¼","ç»“æœ","å› ç´ ","åŸåˆ™","è¦æ±‚",
    "å¯¹è±¡","è´£ä»»","åˆ¶åº¦","æ ‡å‡†","æªæ–½","æƒ…å½¢","ç›®çš„","æ€§è´¨","æ¦‚å¿µ","å…³ç³»","ä¾æ®","æ¡ä»¶",
    "èŒƒå›´","ç¨‹åº¦","æ–¹æ³•","æ„è§","å†³å®š","é€šçŸ¥","å…¬å‘Š","ææ–™","è¯æ®","äº‹å®","ç†ç”±","ç»“è®º",
}
BAD_SUFFIX = ("æ–¹é¢","é—®é¢˜","æƒ…å†µ","è¿‡ç¨‹","å†…å®¹","æ–¹å¼","ç»“æœ","å› ç´ ","åŸåˆ™","è¦æ±‚","åˆ¶åº¦","å…³ç³»","æ ‡å‡†","æªæ–½","æƒ…å½¢")
MAX_MENTION_LEN = 20

# åˆ—è¡¨åˆ†éš”ç¬¦ï¼šç”¨äºæŠŠâ€œAã€Bã€Câ€æ‹†æˆå¤šä¸ªå®ä½“
LIST_SEPS = ("ã€", "ï¼Œ", ",", "ï¼›", ";", "/", "ï¼")


def load_sentences_from_tsv(tsv_path: str) -> List[Dict[str, Any]]:
    """
    åŠ è½½ Step2 ç”Ÿæˆçš„ TSVï¼šsentence_id | page_no | text
    """
    if not os.path.exists(tsv_path):
        raise FileNotFoundError(tsv_path)

    sentences = []
    with open(tsv_path, "r", encoding="utf-8") as f:
        _ = f.readline()  # header
        for line in f:
            parts = line.rstrip("\n").split("\t")
            if len(parts) < 3:
                continue
            sentence_id, page_no, text = parts[0], parts[1], parts[2]
            sentences.append({"sentence_id": sentence_id, "page_no": int(page_no), "text": text})
    return sentences


def clean_mention(mention: str) -> str:
    """
    æ¸…æ´—å®ä½“å­—ç¬¦ä¸²ï¼š
    - å»æ‰ç©ºç™½
    - å»æ‰å¤–å›´ç¬¦å·
    - å»æ‰å‰å¯¼â€œå’Œ/çš„â€ï¼ˆé•¿åº¦è¶³å¤Ÿæ—¶ï¼‰
    - ç‰¹åˆ«å¤„ç†ï¼šåˆ‘æ³•ä¿®æ­£æ¡ˆæ‹¬å·
    """
    m = re.sub(r"\s+", "", mention)
    m = m.strip("ã€Šã€‹ã€Œã€ã€ã€‘[]()ï¼ˆï¼‰\"'â€œâ€â€˜â€™")

    if len(m) > 3 and m[0] in ("å’Œ", "çš„"):
        m = m[1:]

    if m.startswith("åˆ‘æ³•ä¿®æ­£æ¡ˆ(") and not (m.endswith(")") or m.endswith("ï¼‰")):
        m = m + ")"
    if m.startswith("åˆ‘æ³•ä¿®æ­£æ¡ˆï¼ˆ") and not (m.endswith(")") or m.endswith("ï¼‰")):
        m = m + "ï¼‰"

    return m


def is_pure_numeric(mention: str) -> bool:
    if not mention:
        return False
    return all(ch in CH_NUMERIC_CHARS for ch in mention)


def is_bad_mention(mention: str, ent_type: str) -> bool:
    """
    åƒåœ¾å®ä½“è¿‡æ»¤ï¼šé•¿åº¦/çº¯æ•°å­—/æ³›åŒ–è¯/æ³›åŒ–åç¼€/è¿‡é•¿ç‰‡æ®µ
    """
    if len(mention) < min_char_len:
        return True
    if len(mention) > MAX_MENTION_LEN:
        return True
    if is_pure_numeric(mention):
        return True

    # çº¯æ ‡ç‚¹æˆ–éå¸¸è§„ç¬¦å·
    if not re.search(r"[\u4e00-\u9fffA-Za-z0-9]", mention):
        return True

    # æ³›åŒ–è¯ï¼ˆå¼ºè¿‡æ»¤ï¼‰
    if mention in BAD_GENERIC:
        return True
    if mention.endswith(BAD_SUFFIX) and len(mention) <= 6:
        return True

    # book ç±»ç‰¹æ®Šè§„åˆ™ï¼ˆå…¼å®¹å¤§å°å†™ï¼‰
    if ent_type.lower() == "book":
        if len(mention) < 3:
            return True
        if mention[0] in ("çš„", "å’Œ") and len(mention) <= 4:
            return True
        if mention in ("å†³å®š", "ä¿®æ­£æ¡ˆ"):
            return True

    return False


def _find_span(text: str, mention: str) -> Optional[Tuple[int, int]]:
    start = text.find(mention)
    if start == -1:
        return None
    return start, start + len(mention)


def _extract_json(content: str) -> Optional[Dict[str, Any]]:
    """
    ä»æ¨¡å‹è¾“å‡ºä¸­å°½é‡æå– JSONï¼šå–ç¬¬ä¸€ä¸ª { åˆ°æœ€åä¸€ä¸ª } ä¹‹é—´
    """
    content = content.strip()
    l = content.find("{")
    r = content.rfind("}")
    if l == -1 or r == -1 or r <= l:
        return None
    try:
        return json.loads(content[l:r + 1])
    except Exception:
        return None


def _llm_extract_mentions(text: str) -> List[Dict[str, str]]:
    """
    è°ƒç”¨ LLM æŠ½å®ä½“ï¼šè¿”å› [{'mention': '...', 'ent_type': '...'}, ...]
    """
    system_prompt = (
        "ä½ æ˜¯ä¸­æ–‡ä¿¡æ¯æŠ½å–åŠ©æ‰‹ã€‚è¯·ä»ç»™å®šå¥å­ä¸­æŠ½å–å®ä½“ã€‚\n"
        "è¾“å‡ºå¿…é¡»æ˜¯ä¸¥æ ¼ JSONï¼ˆä¸è¦ Markdownï¼Œä¸è¦è§£é‡Šï¼‰ï¼Œæ ¼å¼ï¼š\n"
        "{\"entities\": [{\"mention\": \"...\", \"ent_type\": \"...\"}, ...]}\n"
        "è§„åˆ™ï¼š\n"
        "1) mention å¿…é¡»æ˜¯åŸå¥ä¸­çš„è¿ç»­å­ä¸²ï¼ŒåŸæ–‡å¤åˆ¶ï¼Œä¸å¾—æ”¹å†™/æ¦‚æ‹¬ã€‚\n"
        "2) ä¸è¦è¾“å‡ºçº¯æ•°å­—ï¼ˆå¦‚â€œ2010â€â€œåäºŒâ€ï¼‰ã€‚\n"
        "3) ent_type åªèƒ½ä»ä»¥ä¸‹é›†åˆé€‰ä¸€ä¸ªï¼šPerson, Org, Law, Crime, Location, Time, Book, Concept, Otherã€‚\n"
        "4) ä¸è¦è¾“å‡ºæ³›åŒ–åè¯ï¼ˆå¦‚ï¼šè¡Œä¸º/è§„å®š/æƒ…å†µ/æ–¹é¢/é—®é¢˜/è¿‡ç¨‹/å†…å®¹/æ–¹å¼/ç»“æœ/å› ç´ /åŸåˆ™/è¦æ±‚/åˆ¶åº¦/å…³ç³»ç­‰ï¼‰ã€‚\n"
        f"5) æ¯ä¸ªå¥å­æœ€å¤šè¾“å‡º {MAX_ENT_PER_SENT} ä¸ªå®ä½“ï¼ŒæŒ‰é‡è¦æ€§ä»é«˜åˆ°ä½æ’åºã€‚\n"
        "6) å®å¯å°‘æŠ½ï¼Œä¸è¦èƒ¡ç¼–ã€‚\n"
    )

    resp = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"å¥å­ï¼š{text}"},
        ],
        temperature=0.1,
    )
    content = (resp.choices[0].message.content or "").strip()
    data = _extract_json(content)
    if not data:
        return []

    entities = data.get("entities", [])
    if not isinstance(entities, list):
        return []

    out = []
    for e in entities:
        if not isinstance(e, dict):
            continue
        mention = str(e.get("mention", "")).strip()
        ent_type = str(e.get("ent_type", "Other")).strip()
        if mention:
            out.append({"mention": mention, "ent_type": ent_type})
    return out


def _split_list_mention(mention: str, ent_type: str) -> List[str]:
    """
    æŠŠç±»ä¼¼ 'æœ€é«˜äººæ°‘æ³•é™¢ã€æœ€é«˜äººæ°‘æ£€å¯Ÿé™¢' æ‹†åˆ†æˆå¤šä¸ªå®ä½“
    - åªåœ¨å‡ºç° LIST_SEPS æ—¶æ‹†
    - æ‹†å®Œåš clean_mention
    """
    if not mention:
        return []
    if not any(sep in mention for sep in LIST_SEPS):
        return [mention]

    # å¯¹å°‘æ•°â€œç¡®å®æ˜¯å›ºå®šçŸ­è¯­â€çš„æƒ…å†µï¼Œé¿å…è¯¯æ‹†ï¼ˆä½ å¯ä»¥ç»§ç»­æ‰©å……ï¼‰
    no_split_whitelist = {"ç½ªåˆ‘æ³•å®šåŸåˆ™"}  # ç¤ºä¾‹
    if mention in no_split_whitelist:
        return [mention]

    parts = [mention]
    for sep in LIST_SEPS:
        new_parts = []
        for p in parts:
            new_parts.extend(p.split(sep))
        parts = new_parts

    parts = [clean_mention(p) for p in parts]
    parts = [p for p in parts if p]  # å»ç©º
    # è¿‡æ»¤å¤ªçŸ­çš„ç¢ç‰‡
    parts = [p for p in parts if len(p) >= min_char_len]
    # å»é‡ä¿åº
    seen = set()
    dedup = []
    for p in parts:
        if p not in seen:
            seen.add(p)
            dedup.append(p)
    return dedup if dedup else [mention]


def postprocess_entities(raw_entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    åå¤„ç†ï¼š
    1) å¥å­å†…å»é‡å ï¼ˆåŒ ent_type æ¯”é•¿åº¦/ç½®ä¿¡åº¦ï¼‰
    2) å…¨å±€æŒ‰ (page_no, mention, ent_type) å»é‡
    """
    ents_by_sent = defaultdict(list)
    for e in raw_entities:
        ents_by_sent[e["sentence_id"]].append(e)

    cleaned = []
    for sent_id, ents in ents_by_sent.items():
        ents_sorted = sorted(
            ents,
            key=lambda x: (x["start_char"], -(x["end_char"] - x["start_char"]))
        )

        kept = []
        for e in ents_sorted:
            overlap = False
            for k in list(kept):
                if e["ent_type"] == k["ent_type"]:
                    if not (e["end_char"] <= k["start_char"] or e["start_char"] >= k["end_char"]):
                        len_e = e["end_char"] - e["start_char"]
                        len_k = k["end_char"] - k["start_char"]
                        if len_e < len_k:
                            overlap = True
                            break
                        elif len_e == len_k and e["confidence"] <= k["confidence"]:
                            overlap = True
                            break
                        else:
                            kept.remove(k)
                            break
            if not overlap:
                kept.append(e)

        cleaned.extend(kept)

    unique = []
    seen = set()
    for e in cleaned:
        key = (e["page_no"], e["mention"], e["ent_type"])
        if key in seen:
            continue
        seen.add(key)
        unique.append(e)

    return unique


def run_ner(sentences: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    ä½¿ç”¨ LLM æ‰§è¡Œå®ä½“æŠ½å–ï¼Œå¹¶åšæ¸…æ´—/è¿‡æ»¤/å»é‡ï¼Œè¾“å‡ºå­—æ®µä¿æŒä¸å˜
    """
    print(f"ğŸ¤– ä½¿ç”¨ LLM æ¨¡å‹æŠ½å®ä½“ï¼š{MODEL_NAME}\n")

    raw_entities = []
    ent_id = 1

    for idx, s in enumerate(sentences, start=1):
        text = s["text"]
        if not text.strip():
            continue

        llm_ents = _llm_extract_mentions(text)

        # âœ… å¥å­çº§æˆªæ–­ï¼šå³ä½¿æ¨¡å‹è¾“å‡ºå¾ˆå¤šï¼Œä¹Ÿåªä¿ç•™å‰ MAX_ENT_PER_SENT ä¸ª
        if len(llm_ents) > MAX_ENT_PER_SENT:
            llm_ents = llm_ents[:MAX_ENT_PER_SENT]

        for r in llm_ents:
            raw_mention = r["mention"]
            ent_type = r.get("ent_type", "Other") or "Other"

            # å…ˆæ¸…æ´—
            cleaned_m = clean_mention(raw_mention)

            # âœ… åˆ—è¡¨æ‹†åˆ†ï¼ˆæ‹†å‡ºå¤šä¸ªå®ä½“ï¼‰
            mentions = _split_list_mention(cleaned_m, ent_type)

            for mention in mentions:
                # è¿‡æ»¤
                if is_bad_mention(mention, ent_type):
                    continue

                span = _find_span(text, mention)
                if not span:
                    # æ¨¡å‹æ²¡éµå®ˆâ€œå­ä¸²â€è§„åˆ™æˆ–æ‹†åˆ†åå®šä½å¤±è´¥ï¼Œè·³è¿‡
                    continue

                start_char, end_char = span
                raw_entities.append(
                    {
                        "entity_id": f"e{ent_id:05d}",
                        "sentence_id": s["sentence_id"],
                        "page_no": s["page_no"],
                        "mention": mention,
                        "start_char": int(start_char),
                        "end_char": int(end_char),
                        "ent_type": ent_type,
                        "confidence": float(DEFAULT_CONF),
                    }
                )
                ent_id += 1

        if idx % 50 == 0:
            print(f"â€¦å·²å¤„ç† {idx}/{len(sentences)} å¥ï¼Œå½“å‰å®ä½“æ•°ï¼š{len(raw_entities)}")

    print(f"\nğŸ§¹ LLM åŸå§‹å®ä½“æ•°ï¼š{len(raw_entities)}ï¼Œå¼€å§‹åšé‡å /é‡å¤è¿‡æ»¤...")
    final_entities = postprocess_entities(raw_entities)
    print(f"âœ… è¿‡æ»¤åå®ä½“æ•°ï¼š{len(final_entities)}")

    return final_entities


def save_entities(entities: List[Dict[str, Any]], output_path: str) -> None:
    """
    è¾“å‡º TSVï¼š
    entity_id | sentence_id | page_no | mention | start_char | end_char | ent_type | confidence
    """
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    with open(output_path, "w", encoding="utf-8", newline="") as f:
        f.write("entity_id\tsentence_id\tpage_no\tmention\tstart_char\tend_char\tent_type\tconfidence\n")
        for e in entities:
            f.write(
                f"{e['entity_id']}\t{e['sentence_id']}\t{e['page_no']}\t"
                f"{e['mention']}\t{e['start_char']}\t{e['end_char']}\t"
                f"{e['ent_type']}\t{e['confidence']:.4f}\n"
            )

    print(f"âœ… å®ä½“åˆ—è¡¨å·²ä¿å­˜åˆ°ï¼š{output_path}")
    print(f"ğŸ“Œ å…±æŠ½å–å®ä½“æ•°é‡ï¼š{len(entities)}")


def main():
    sentences = load_sentences_from_tsv(sent_tsv_path)
    print(f"ğŸ“„ å·²åŠ è½½å¥å­æ•°é‡ï¼š{len(sentences)}")

    entities = run_ner(sentences)

    print("\nğŸ“Œ å®ä½“ç¤ºä¾‹ï¼ˆå‰10æ¡ï¼‰ï¼š")
    for e in entities[:10]:
        print(f"{e['entity_id']}: {e['mention']} ({e['ent_type']}, p{e['page_no']}, score={e['confidence']:.2f})")

    save_entities(entities, output_entity_path)


if __name__ == "__main__":
    main()
